{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import argmax\n",
    "import sys\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "# library options\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras import backend\n",
    " \n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Dropout Layer\n",
    "class MonteCarloDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(pd.read_parquet('data/X_train.parquet')).astype('float')\n",
    "X_test = np.array(pd.read_parquet('data/X_test.parquet')).astype('float')\n",
    "\n",
    "y_test_class = np.array(pd.read_parquet('data/y_test.parquet')['sum_payments_package_key_ltv_class'])\n",
    "y_train_class = np.array(pd.read_parquet('data/y_train.parquet')['sum_payments_package_key_ltv_class'])                        \n",
    "#y_test_class = np.array(pd.read_parquet('data/y_test.parquet')['sum_payments_package_key_ltv_class'].apply(lambda x: 0 if x == 'low' else 1 if x == \"medium\" else 2 if x == \"high\" else -1)).astype('float')\n",
    "#y_train_class = np.array(pd.read_parquet('data/y_test.parquet')['sum_payments_package_key_ltv_class'].apply(lambda x: 0 if x == 'low' else 1 if x == \"medium\" else 2 if x == \"high\" else -1)).astype('float')\n",
    "y_test_reg = np.array(pd.read_parquet('data/y_test.parquet')['sum_payments_package_key_ltv']).astype('float')\n",
    "y_train_reg = np.array(pd.read_parquet('data/y_train.parquet')['sum_payments_package_key_ltv']).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Data before Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (111791, 180)\n",
      "Testing Features Shape: (27948, 180)\n",
      "Training Labels Regression Shape: (111791,)\n",
      "Testing Labels Regression Shape: (27948,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', X_train.shape)\n",
    "print('Testing Features Shape:', X_test.shape)\n",
    "print('Training Labels Regression Shape:', y_train_reg.shape)\n",
    "print('Testing Labels Regression Shape:', y_test_reg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # layer 01\n",
    "    layer_01 = hp.Choice('layer_01', values = ['False','True'])\n",
    "    if layer_01:\n",
    "        units_01 = hp.Int('units_01', min_value=64, max_value=1024, step=32)\n",
    "        activation_01 = hp.Choice('activation_01', values = ['LeakyReLU','relu','sigmoid','tanh'])\n",
    "        kernel_initializer_01 = hp.Choice('kernel_01', values = ['he_uniform', 'he_normal', 'glorot_uniform', 'glorot_normal'])\n",
    "        model.add(Dense(units_01, activation = activation_01, kernel_initializer=kernel_initializer_01, input_shape = (X_train.shape[1],)))\n",
    "    \n",
    "    batch_01 = hp.Choice('batch_01', values = ['False','True'])\n",
    "    if batch_01:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    drop_01 = hp.Choice('drop_01', values = ['False','drop','Monte'])\n",
    "    if drop_01 == 'drop':\n",
    "        dropout_01 = hp.Float('dropout_01', min_value=0.1, max_value=0.6, step=0.1)\n",
    "        model.add(Dropout(dropout_01))\n",
    "    elif drop_01 == 'Monte':\n",
    "        monte_01 = hp.Float('monte_01', min_value=0.1, max_value=0.6, step=0.1)\n",
    "        model.add(MonteCarloDropout(monte_01))\n",
    "    \n",
    "    # layer 02\n",
    "    layer_02 = hp.Choice('layer_02', values = ['False','True'])\n",
    "    if layer_02:\n",
    "        units_02 = hp.Int('units_02', min_value = 32, max_value = 512, step=32)\n",
    "        activation_02 = hp.Choice('activation_02', values = ['LeakyReLU','relu','sigmoid','tanh'])\n",
    "        kernel_initializer_02 = hp.Choice('kernel_02', values = ['he_uniform', 'he_normal', 'glorot_uniform', 'glorot_normal'])\n",
    "        model.add(Dense(units_02, activation = activation_02, kernel_initializer=kernel_initializer_02))\n",
    "    \n",
    "    batch_02 = hp.Choice('batch_02', values = ['False','True'])\n",
    "    if batch_02:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    drop_02 = hp.Choice('drop_02', values = ['False','drop','Monte'])\n",
    "    if drop_02:\n",
    "        dropout_02 = hp.Float('dropout_02', min_value=0.1, max_value=0.6, step=0.1)\n",
    "        model.add(Dropout(dropout_02))\n",
    "    elif drop_02 == 'Monte':\n",
    "        monte_02 = hp.Float('monte_02', min_value=0.1, max_value=0.6, step=0.1)\n",
    "        model.add(MonteCarloDropout(monte_02))\n",
    "    \n",
    "    # layer 03\n",
    "    layer_03 = hp.Choice('layer_03', values = ['False','True'])\n",
    "    if layer_03:\n",
    "        units_03 = hp.Int('units_03', min_value = 32, max_value = 512, step=32)\n",
    "        activation_03 = hp.Choice('activation_03', values = ['LeakyReLU','relu','sigmoid','tanh'])\n",
    "        kernel_initializer_03 = hp.Choice('kernel_03', values = ['he_uniform', 'he_normal', 'glorot_uniform', 'glorot_normal'])\n",
    "        model.add(Dense(units_03, activation = activation_03, kernel_initializer=kernel_initializer_03))\n",
    "    \n",
    "    drop_03 = hp.Choice('drop_03', values = ['False','drop','Monte'])\n",
    "    if drop_03:\n",
    "        dropout_03 = hp.Float('dropout_03', min_value=0.1, max_value=0.6, step=0.1)\n",
    "        model.add(Dropout(dropout_03))\n",
    "    elif drop_03 == 'Monte':\n",
    "        monte_03 = hp.Float('monte_03', min_value=0.1, max_value=0.6, step=0.1)\n",
    "        model.add(MonteCarloDropout(monte_03))\n",
    "    \n",
    "    batch_03 = hp.Choice('batch_03', values = ['False','True'])\n",
    "    if batch_03:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    # layer 04\n",
    "    layer_04 = hp.Choice('layer_04', values = ['False','True'])\n",
    "    if layer_04:\n",
    "        units_04 = hp.Int('units_04', min_value = 16, max_value = 512, step=32)\n",
    "        activation_04 = hp.Choice('activation_04', values = ['LeakyReLU','relu','sigmoid','tanh'])\n",
    "        kernel_initializer_04 = hp.Choice('kernel_04', values = ['he_uniform', 'he_normal', 'glorot_uniform', 'glorot_normal'])\n",
    "        model.add(Dense(units_04, activation = activation_04, kernel_initializer=kernel_initializer_04))\n",
    "    \n",
    "    drop_04 = hp.Choice('drop_04', values = ['False','drop','Monte'])\n",
    "    if drop_04:\n",
    "        dropout_04 = hp.Float('dropout_04', min_value=0.1, max_value=0.6, step=0.1)\n",
    "        model.add(Dropout(dropout_04))\n",
    "    elif drop_04 == 'Monte':\n",
    "        monte_04 = hp.Float('monte_04', min_value=0.1, max_value=0.6, step=0.1)\n",
    "        model.add(MonteCarloDropout(monte_04))\n",
    "        \n",
    "    batch_04 = hp.Choice('batch_04', values = ['False','True'])\n",
    "    if batch_04:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    # layer 05\n",
    "    layer_05 = hp.Choice('layer_05', values = ['False','True'])\n",
    "    if layer_05:\n",
    "        units_05 = hp.Int('units_05', min_value = 16, max_value = 256, step=32)\n",
    "        activation_05 = hp.Choice('activation_05', values = ['LeakyReLU','relu','sigmoid','tanh'])\n",
    "        kernel_initializer_05 = hp.Choice('kernel_05', values = ['he_uniform', 'he_normal', 'glorot_uniform', 'glorot_normal'])\n",
    "        model.add(Dense(units_05, activation = activation_05, kernel_initializer=kernel_initializer_05))\n",
    "    \n",
    "    drop_05 = hp.Choice('drop_05', values = ['False','drop','Monte'])\n",
    "    if drop_05:\n",
    "        dropout_05 = hp.Float('dropout_05', min_value=0.1, max_value=0.6, step=0.1)\n",
    "        model.add(Dropout(dropout_05))\n",
    "    elif drop_05 == 'Monte':\n",
    "        monte_05 = hp.Float('monte_05', min_value=0.1, max_value=0.6, step=0.1)\n",
    "        model.add(MonteCarloDropout(monte_05))\n",
    "        \n",
    "    batch_05 = hp.Choice('batch_05', values = ['False','True'])\n",
    "    if batch_05:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    # layer 06\n",
    "    layer_06 = hp.Choice('layer_06', values = ['False','True'])\n",
    "    if layer_06:\n",
    "        units_06 = hp.Int('units_06', min_value = 4, max_value = 16, step=4)\n",
    "        activation_06 = hp.Choice('activation_06', values = ['LeakyReLU','relu','sigmoid','tanh'])\n",
    "        kernel_initializer_06 = hp.Choice('kernel_06', values = ['he_uniform', 'he_normal', 'glorot_uniform', 'glorot_normal'])\n",
    "        model.add(Dense(units_06, activation = activation_06, kernel_initializer=kernel_initializer_06))\n",
    "        \n",
    "    # layer 07\n",
    "    layer_07 = hp.Choice('layer_07', values = ['False','True'])\n",
    "    if layer_07:\n",
    "        units_07 = hp.Int('units_07', min_value = 4, max_value = 16, step=4)\n",
    "        activation_07 = hp.Choice('activation_07', values = ['LeakyReLU','relu','sigmoid','tanh'])\n",
    "        kernel_initializer_07 = hp.Choice('kernel_07', values = ['he_uniform', 'he_normal', 'glorot_uniform', 'glorot_normal'])\n",
    "        model.add(Dense(units_07, activation = activation_07, kernel_initializer=kernel_initializer_07))    \n",
    "    \n",
    "    # layer 08\n",
    "    layer_08 = hp.Choice('layer_08', values = ['False','True'])\n",
    "    if layer_08:\n",
    "        units_08 = hp.Int('units_08', min_value = 4, max_value = 16, step=4)\n",
    "        activation_08 = hp.Choice('activation_08', values = ['LeakyReLU','relu','sigmoid','tanh'])\n",
    "        kernel_initializer_08 = hp.Choice('kernel_08', values = ['he_uniform', 'he_normal', 'glorot_uniform', 'glorot_normal'])\n",
    "        model.add(Dense(units_08, activation = activation_08, kernel_initializer=kernel_initializer_08))\n",
    "        \n",
    "    # layer 09\n",
    "    layer_09 = hp.Choice('layer_06', values = ['False','True'])\n",
    "    if layer_09:\n",
    "        units_09 = hp.Int('units_06', min_value = 4, max_value = 16, step=4)\n",
    "        activation_09 = hp.Choice('activation_09', values = ['LeakyReLU','relu','sigmoid','tanh'])\n",
    "        kernel_initializer_09 = hp.Choice('kernel_09', values = ['he_uniform', 'he_normal', 'glorot_uniform', 'glorot_normal'])\n",
    "        model.add(Dense(units_09, activation = activation_09, kernel_initializer=kernel_initializer_09))\n",
    "        \n",
    "    # layer 10\n",
    "    layer_10 = hp.Choice('layer_10', values = ['False','True'])\n",
    "    if layer_10:\n",
    "        units_10 = hp.Int('units_10', min_value = 4, max_value = 16, step=4)\n",
    "        activation_10 = hp.Choice('activation_10', values = ['LeakyReLU','relu','sigmoid','tanh'])\n",
    "        kernel_initializer_10 = hp.Choice('kernel_10', values = ['he_uniform', 'he_normal', 'glorot_uniform', 'glorot_normal'])\n",
    "        model.add(Dense(units_10, activation = activation_10, kernel_initializer=kernel_initializer_10))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss = 'mse', metrics = ['mse', 'mae',rmse,'mape','msle'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./MLP_Regression_02/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from ./MLP_Regression_02/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Select Tuner Hyperband\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = 'mse',\n",
    "                     max_epochs = 40,\n",
    "                     factor = 6,\n",
    "                     project_name = 'MLP_Regression_02')\n",
    "\n",
    "stop_early = callbacks.EarlyStopping(monitor='loss', patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "The hyperparameter search is complete. The optimal model is:\n",
      "<keras_tuner.engine.hyperparameters.HyperParameters object at 0x7fc203672400>\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train_reg, epochs=100, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print('The hyperparameter search is complete. The optimal model is:')\n",
    "print(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./MLP_Regression_02\n",
      "Showing 10 best trials\n",
      "Objective(name='mse', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "layer_01: False\n",
      "units_01: 992\n",
      "activation_01: tanh\n",
      "kernel_01: he_uniform\n",
      "batch_01: False\n",
      "drop_01: drop\n",
      "layer_02: True\n",
      "units_02: 128\n",
      "activation_02: LeakyReLU\n",
      "kernel_02: he_uniform\n",
      "batch_02: True\n",
      "drop_02: drop\n",
      "dropout_02: 0.4\n",
      "layer_03: True\n",
      "units_03: 256\n",
      "activation_03: sigmoid\n",
      "kernel_03: glorot_normal\n",
      "drop_03: drop\n",
      "dropout_03: 0.30000000000000004\n",
      "batch_03: False\n",
      "layer_04: False\n",
      "units_04: 240\n",
      "activation_04: relu\n",
      "kernel_04: glorot_normal\n",
      "drop_04: False\n",
      "dropout_04: 0.6\n",
      "batch_04: False\n",
      "layer_05: False\n",
      "units_05: 144\n",
      "activation_05: relu\n",
      "kernel_05: glorot_uniform\n",
      "drop_05: False\n",
      "dropout_05: 0.6\n",
      "batch_05: True\n",
      "layer_06: True\n",
      "units_06: 12\n",
      "activation_06: relu\n",
      "kernel_06: glorot_normal\n",
      "layer_07: True\n",
      "units_07: 8\n",
      "activation_07: tanh\n",
      "kernel_07: glorot_normal\n",
      "layer_08: False\n",
      "units_08: 4\n",
      "activation_08: sigmoid\n",
      "kernel_08: he_normal\n",
      "activation_09: relu\n",
      "kernel_09: he_uniform\n",
      "layer_10: False\n",
      "units_10: 8\n",
      "activation_10: relu\n",
      "kernel_10: glorot_normal\n",
      "learning_rate: 0.001\n",
      "dropout_01: 0.30000000000000004\n",
      "monte_01: 0.6\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 677.3482666015625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "layer_01: False\n",
      "units_01: 224\n",
      "activation_01: sigmoid\n",
      "kernel_01: glorot_normal\n",
      "batch_01: False\n",
      "drop_01: drop\n",
      "layer_02: False\n",
      "units_02: 96\n",
      "activation_02: sigmoid\n",
      "kernel_02: glorot_uniform\n",
      "batch_02: False\n",
      "drop_02: Monte\n",
      "dropout_02: 0.4\n",
      "layer_03: False\n",
      "units_03: 448\n",
      "activation_03: relu\n",
      "kernel_03: glorot_uniform\n",
      "drop_03: drop\n",
      "dropout_03: 0.5\n",
      "batch_03: False\n",
      "layer_04: True\n",
      "units_04: 144\n",
      "activation_04: LeakyReLU\n",
      "kernel_04: he_normal\n",
      "drop_04: drop\n",
      "dropout_04: 0.2\n",
      "batch_04: True\n",
      "layer_05: False\n",
      "units_05: 48\n",
      "activation_05: relu\n",
      "kernel_05: he_normal\n",
      "drop_05: False\n",
      "dropout_05: 0.30000000000000004\n",
      "batch_05: False\n",
      "layer_06: True\n",
      "units_06: 12\n",
      "activation_06: LeakyReLU\n",
      "kernel_06: he_uniform\n",
      "layer_07: True\n",
      "units_07: 8\n",
      "activation_07: sigmoid\n",
      "kernel_07: glorot_uniform\n",
      "layer_08: False\n",
      "units_08: 16\n",
      "activation_08: tanh\n",
      "kernel_08: he_uniform\n",
      "activation_09: LeakyReLU\n",
      "kernel_09: he_uniform\n",
      "layer_10: False\n",
      "units_10: 4\n",
      "activation_10: relu\n",
      "kernel_10: he_normal\n",
      "learning_rate: 0.001\n",
      "dropout_01: 0.30000000000000004\n",
      "monte_01: 0.1\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: e219fd9a59ba791940a39df467927064\n",
      "Score: 707.607421875\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "layer_01: False\n",
      "units_01: 800\n",
      "activation_01: sigmoid\n",
      "kernel_01: he_uniform\n",
      "batch_01: True\n",
      "drop_01: False\n",
      "layer_02: False\n",
      "units_02: 480\n",
      "activation_02: LeakyReLU\n",
      "kernel_02: glorot_uniform\n",
      "batch_02: False\n",
      "drop_02: Monte\n",
      "dropout_02: 0.5\n",
      "layer_03: True\n",
      "units_03: 32\n",
      "activation_03: relu\n",
      "kernel_03: he_uniform\n",
      "drop_03: Monte\n",
      "dropout_03: 0.2\n",
      "batch_03: True\n",
      "layer_04: False\n",
      "units_04: 144\n",
      "activation_04: tanh\n",
      "kernel_04: glorot_normal\n",
      "drop_04: Monte\n",
      "dropout_04: 0.30000000000000004\n",
      "batch_04: False\n",
      "layer_05: False\n",
      "units_05: 208\n",
      "activation_05: tanh\n",
      "kernel_05: he_normal\n",
      "drop_05: False\n",
      "dropout_05: 0.2\n",
      "batch_05: True\n",
      "layer_06: True\n",
      "units_06: 12\n",
      "activation_06: tanh\n",
      "kernel_06: glorot_uniform\n",
      "layer_07: False\n",
      "units_07: 8\n",
      "activation_07: relu\n",
      "kernel_07: he_uniform\n",
      "layer_08: False\n",
      "units_08: 16\n",
      "activation_08: relu\n",
      "kernel_08: glorot_uniform\n",
      "activation_09: sigmoid\n",
      "kernel_09: he_normal\n",
      "layer_10: True\n",
      "units_10: 12\n",
      "activation_10: LeakyReLU\n",
      "kernel_10: glorot_normal\n",
      "learning_rate: 0.001\n",
      "dropout_01: 0.4\n",
      "monte_01: 0.6\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 097ade20c661857e5def10e91406ae2e\n",
      "Score: 726.1967163085938\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "layer_01: True\n",
      "units_01: 256\n",
      "activation_01: relu\n",
      "kernel_01: he_uniform\n",
      "batch_01: False\n",
      "drop_01: drop\n",
      "layer_02: True\n",
      "units_02: 192\n",
      "activation_02: sigmoid\n",
      "kernel_02: he_normal\n",
      "batch_02: True\n",
      "drop_02: Monte\n",
      "dropout_02: 0.30000000000000004\n",
      "layer_03: True\n",
      "units_03: 416\n",
      "activation_03: sigmoid\n",
      "kernel_03: he_uniform\n",
      "drop_03: drop\n",
      "dropout_03: 0.30000000000000004\n",
      "batch_03: False\n",
      "layer_04: True\n",
      "units_04: 496\n",
      "activation_04: tanh\n",
      "kernel_04: glorot_normal\n",
      "drop_04: False\n",
      "dropout_04: 0.4\n",
      "batch_04: True\n",
      "layer_05: True\n",
      "units_05: 112\n",
      "activation_05: relu\n",
      "kernel_05: he_normal\n",
      "drop_05: drop\n",
      "dropout_05: 0.1\n",
      "batch_05: True\n",
      "layer_06: False\n",
      "units_06: 4\n",
      "activation_06: LeakyReLU\n",
      "kernel_06: glorot_uniform\n",
      "layer_07: True\n",
      "units_07: 12\n",
      "activation_07: LeakyReLU\n",
      "kernel_07: glorot_normal\n",
      "layer_08: False\n",
      "units_08: 16\n",
      "activation_08: sigmoid\n",
      "kernel_08: he_uniform\n",
      "activation_09: LeakyReLU\n",
      "kernel_09: glorot_normal\n",
      "layer_10: False\n",
      "units_10: 12\n",
      "activation_10: relu\n",
      "kernel_10: he_uniform\n",
      "learning_rate: 0.001\n",
      "dropout_01: 0.4\n",
      "monte_01: 0.30000000000000004\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: d8f842129eaaa2414364e4a319a3fd97\n",
      "Score: 729.0558471679688\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "layer_01: False\n",
      "units_01: 832\n",
      "activation_01: relu\n",
      "kernel_01: glorot_normal\n",
      "batch_01: False\n",
      "drop_01: drop\n",
      "layer_02: False\n",
      "units_02: 288\n",
      "activation_02: tanh\n",
      "kernel_02: glorot_uniform\n",
      "batch_02: True\n",
      "drop_02: Monte\n",
      "dropout_02: 0.30000000000000004\n",
      "layer_03: False\n",
      "units_03: 416\n",
      "activation_03: relu\n",
      "kernel_03: he_normal\n",
      "drop_03: False\n",
      "dropout_03: 0.5\n",
      "batch_03: False\n",
      "layer_04: True\n",
      "units_04: 432\n",
      "activation_04: tanh\n",
      "kernel_04: glorot_normal\n",
      "drop_04: drop\n",
      "dropout_04: 0.2\n",
      "batch_04: True\n",
      "layer_05: True\n",
      "units_05: 112\n",
      "activation_05: LeakyReLU\n",
      "kernel_05: glorot_uniform\n",
      "drop_05: drop\n",
      "dropout_05: 0.2\n",
      "batch_05: True\n",
      "layer_06: True\n",
      "units_06: 16\n",
      "activation_06: tanh\n",
      "kernel_06: glorot_normal\n",
      "layer_07: True\n",
      "units_07: 16\n",
      "activation_07: tanh\n",
      "kernel_07: glorot_uniform\n",
      "layer_08: False\n",
      "units_08: 8\n",
      "activation_08: tanh\n",
      "kernel_08: he_uniform\n",
      "activation_09: sigmoid\n",
      "kernel_09: glorot_normal\n",
      "layer_10: True\n",
      "units_10: 8\n",
      "activation_10: relu\n",
      "kernel_10: he_normal\n",
      "learning_rate: 0.001\n",
      "dropout_01: 0.1\n",
      "monte_01: 0.2\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 38e82116e45fcc996548459d28129d5b\n",
      "Score: 731.2570190429688\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "layer_01: True\n",
      "units_01: 960\n",
      "activation_01: LeakyReLU\n",
      "kernel_01: he_normal\n",
      "batch_01: True\n",
      "drop_01: Monte\n",
      "layer_02: False\n",
      "units_02: 352\n",
      "activation_02: LeakyReLU\n",
      "kernel_02: glorot_normal\n",
      "batch_02: True\n",
      "drop_02: False\n",
      "dropout_02: 0.1\n",
      "layer_03: False\n",
      "units_03: 224\n",
      "activation_03: sigmoid\n",
      "kernel_03: glorot_uniform\n",
      "drop_03: False\n",
      "dropout_03: 0.2\n",
      "batch_03: True\n",
      "layer_04: False\n",
      "units_04: 208\n",
      "activation_04: sigmoid\n",
      "kernel_04: he_normal\n",
      "drop_04: False\n",
      "dropout_04: 0.4\n",
      "batch_04: True\n",
      "layer_05: False\n",
      "units_05: 48\n",
      "activation_05: LeakyReLU\n",
      "kernel_05: he_normal\n",
      "drop_05: False\n",
      "dropout_05: 0.1\n",
      "batch_05: True\n",
      "layer_06: False\n",
      "units_06: 4\n",
      "activation_06: LeakyReLU\n",
      "kernel_06: glorot_uniform\n",
      "layer_07: True\n",
      "units_07: 8\n",
      "activation_07: relu\n",
      "kernel_07: glorot_uniform\n",
      "layer_08: True\n",
      "units_08: 16\n",
      "activation_08: sigmoid\n",
      "kernel_08: glorot_uniform\n",
      "activation_09: LeakyReLU\n",
      "kernel_09: he_normal\n",
      "layer_10: True\n",
      "units_10: 12\n",
      "activation_10: tanh\n",
      "kernel_10: he_normal\n",
      "learning_rate: 0.001\n",
      "dropout_01: 0.4\n",
      "monte_01: 0.4\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 733.2710571289062\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "layer_01: True\n",
      "units_01: 992\n",
      "activation_01: LeakyReLU\n",
      "kernel_01: he_uniform\n",
      "batch_01: True\n",
      "drop_01: drop\n",
      "layer_02: True\n",
      "units_02: 96\n",
      "activation_02: tanh\n",
      "kernel_02: he_normal\n",
      "batch_02: True\n",
      "drop_02: drop\n",
      "dropout_02: 0.1\n",
      "layer_03: True\n",
      "units_03: 64\n",
      "activation_03: LeakyReLU\n",
      "kernel_03: glorot_normal\n",
      "drop_03: False\n",
      "dropout_03: 0.6\n",
      "batch_03: False\n",
      "layer_04: True\n",
      "units_04: 400\n",
      "activation_04: relu\n",
      "kernel_04: glorot_uniform\n",
      "drop_04: drop\n",
      "dropout_04: 0.4\n",
      "batch_04: False\n",
      "layer_05: False\n",
      "units_05: 208\n",
      "activation_05: relu\n",
      "kernel_05: glorot_uniform\n",
      "drop_05: False\n",
      "dropout_05: 0.30000000000000004\n",
      "batch_05: True\n",
      "layer_06: False\n",
      "units_06: 8\n",
      "activation_06: relu\n",
      "kernel_06: glorot_uniform\n",
      "layer_07: True\n",
      "units_07: 8\n",
      "activation_07: tanh\n",
      "kernel_07: glorot_uniform\n",
      "layer_08: True\n",
      "units_08: 16\n",
      "activation_08: relu\n",
      "kernel_08: glorot_normal\n",
      "activation_09: sigmoid\n",
      "kernel_09: he_uniform\n",
      "layer_10: False\n",
      "units_10: 12\n",
      "activation_10: LeakyReLU\n",
      "kernel_10: glorot_uniform\n",
      "learning_rate: 0.001\n",
      "dropout_01: 0.4\n",
      "monte_01: 0.2\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 759.8583374023438\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "layer_01: False\n",
      "units_01: 832\n",
      "activation_01: relu\n",
      "kernel_01: glorot_normal\n",
      "batch_01: False\n",
      "drop_01: drop\n",
      "layer_02: False\n",
      "units_02: 288\n",
      "activation_02: tanh\n",
      "kernel_02: glorot_uniform\n",
      "batch_02: True\n",
      "drop_02: Monte\n",
      "dropout_02: 0.30000000000000004\n",
      "layer_03: False\n",
      "units_03: 416\n",
      "activation_03: relu\n",
      "kernel_03: he_normal\n",
      "drop_03: False\n",
      "dropout_03: 0.5\n",
      "batch_03: False\n",
      "layer_04: True\n",
      "units_04: 432\n",
      "activation_04: tanh\n",
      "kernel_04: glorot_normal\n",
      "drop_04: drop\n",
      "dropout_04: 0.2\n",
      "batch_04: True\n",
      "layer_05: True\n",
      "units_05: 112\n",
      "activation_05: LeakyReLU\n",
      "kernel_05: glorot_uniform\n",
      "drop_05: drop\n",
      "dropout_05: 0.2\n",
      "batch_05: True\n",
      "layer_06: True\n",
      "units_06: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_06: tanh\n",
      "kernel_06: glorot_normal\n",
      "layer_07: True\n",
      "units_07: 16\n",
      "activation_07: tanh\n",
      "kernel_07: glorot_uniform\n",
      "layer_08: False\n",
      "units_08: 8\n",
      "activation_08: tanh\n",
      "kernel_08: he_uniform\n",
      "activation_09: sigmoid\n",
      "kernel_09: glorot_normal\n",
      "layer_10: True\n",
      "units_10: 8\n",
      "activation_10: relu\n",
      "kernel_10: he_normal\n",
      "learning_rate: 0.001\n",
      "dropout_01: 0.1\n",
      "monte_01: 0.2\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 762.2202758789062\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "layer_01: True\n",
      "units_01: 256\n",
      "activation_01: relu\n",
      "kernel_01: he_uniform\n",
      "batch_01: False\n",
      "drop_01: drop\n",
      "layer_02: True\n",
      "units_02: 192\n",
      "activation_02: sigmoid\n",
      "kernel_02: he_normal\n",
      "batch_02: True\n",
      "drop_02: Monte\n",
      "dropout_02: 0.30000000000000004\n",
      "layer_03: True\n",
      "units_03: 416\n",
      "activation_03: sigmoid\n",
      "kernel_03: he_uniform\n",
      "drop_03: drop\n",
      "dropout_03: 0.30000000000000004\n",
      "batch_03: False\n",
      "layer_04: True\n",
      "units_04: 496\n",
      "activation_04: tanh\n",
      "kernel_04: glorot_normal\n",
      "drop_04: False\n",
      "dropout_04: 0.4\n",
      "batch_04: True\n",
      "layer_05: True\n",
      "units_05: 112\n",
      "activation_05: relu\n",
      "kernel_05: he_normal\n",
      "drop_05: drop\n",
      "dropout_05: 0.1\n",
      "batch_05: True\n",
      "layer_06: False\n",
      "units_06: 4\n",
      "activation_06: LeakyReLU\n",
      "kernel_06: glorot_uniform\n",
      "layer_07: True\n",
      "units_07: 12\n",
      "activation_07: LeakyReLU\n",
      "kernel_07: glorot_normal\n",
      "layer_08: False\n",
      "units_08: 16\n",
      "activation_08: sigmoid\n",
      "kernel_08: he_uniform\n",
      "activation_09: LeakyReLU\n",
      "kernel_09: glorot_normal\n",
      "layer_10: False\n",
      "units_10: 12\n",
      "activation_10: relu\n",
      "kernel_10: he_uniform\n",
      "learning_rate: 0.001\n",
      "dropout_01: 0.4\n",
      "monte_01: 0.30000000000000004\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: a227fcecba50043f8750d00ebcbada53\n",
      "Score: 764.6197509765625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "layer_01: False\n",
      "units_01: 800\n",
      "activation_01: sigmoid\n",
      "kernel_01: he_uniform\n",
      "batch_01: True\n",
      "drop_01: False\n",
      "layer_02: False\n",
      "units_02: 480\n",
      "activation_02: LeakyReLU\n",
      "kernel_02: glorot_uniform\n",
      "batch_02: False\n",
      "drop_02: Monte\n",
      "dropout_02: 0.5\n",
      "layer_03: True\n",
      "units_03: 32\n",
      "activation_03: relu\n",
      "kernel_03: he_uniform\n",
      "drop_03: Monte\n",
      "dropout_03: 0.2\n",
      "batch_03: True\n",
      "layer_04: False\n",
      "units_04: 144\n",
      "activation_04: tanh\n",
      "kernel_04: glorot_normal\n",
      "drop_04: Monte\n",
      "dropout_04: 0.30000000000000004\n",
      "batch_04: False\n",
      "layer_05: False\n",
      "units_05: 208\n",
      "activation_05: tanh\n",
      "kernel_05: he_normal\n",
      "drop_05: False\n",
      "dropout_05: 0.2\n",
      "batch_05: True\n",
      "layer_06: True\n",
      "units_06: 12\n",
      "activation_06: tanh\n",
      "kernel_06: glorot_uniform\n",
      "layer_07: False\n",
      "units_07: 8\n",
      "activation_07: relu\n",
      "kernel_07: he_uniform\n",
      "layer_08: False\n",
      "units_08: 16\n",
      "activation_08: relu\n",
      "kernel_08: glorot_uniform\n",
      "activation_09: sigmoid\n",
      "kernel_09: he_normal\n",
      "layer_10: True\n",
      "units_10: 12\n",
      "activation_10: LeakyReLU\n",
      "kernel_10: glorot_normal\n",
      "learning_rate: 0.001\n",
      "dropout_01: 0.4\n",
      "monte_01: 0.6\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 767.9053344726562\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2341/2341 [==============================] - 18s 7ms/step - loss: 1201.5404 - mse: 1201.5404 - mae: 18.9991 - rmse: 18.9991 - mape: 231.5899 - msle: 1.4051 - val_loss: 828.5510 - val_mse: 828.5510 - val_mae: 15.6648 - val_rmse: 15.6648 - val_mape: 166.6517 - val_msle: 0.7993\n",
      "Epoch 2/100\n",
      "2341/2341 [==============================] - 15s 6ms/step - loss: 846.0843 - mse: 846.0843 - mae: 16.4206 - rmse: 16.4206 - mape: 214.2570 - msle: 0.9619 - val_loss: 782.6606 - val_mse: 782.6606 - val_mae: 16.3411 - val_rmse: 16.3411 - val_mape: 227.3297 - val_msle: 0.9841\n",
      "Epoch 3/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 808.6876 - mse: 808.6876 - mae: 15.9428 - rmse: 15.9428 - mape: 208.8731 - msle: 0.9393 - val_loss: 769.0543 - val_mse: 769.0543 - val_mae: 15.3487 - val_rmse: 15.3487 - val_mape: 179.8793 - val_msle: 0.8165\n",
      "Epoch 4/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 792.6050 - mse: 792.6050 - mae: 15.6700 - rmse: 15.6700 - mape: 198.8147 - msle: 0.9023 - val_loss: 763.4987 - val_mse: 763.4987 - val_mae: 14.8384 - val_rmse: 14.8384 - val_mape: 160.5051 - val_msle: 0.7558\n",
      "Epoch 5/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 775.7543 - mse: 775.7543 - mae: 15.4411 - rmse: 15.4411 - mape: 193.6178 - msle: 0.8846 - val_loss: 758.3436 - val_mse: 758.3436 - val_mae: 14.5886 - val_rmse: 14.5886 - val_mape: 161.4148 - val_msle: 0.7658\n",
      "Epoch 6/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 764.8564 - mse: 764.8564 - mae: 15.2449 - rmse: 15.2449 - mape: 193.2010 - msle: 0.8786 - val_loss: 765.0192 - val_mse: 765.0192 - val_mae: 15.1518 - val_rmse: 15.1518 - val_mape: 177.3335 - val_msle: 0.8075\n",
      "Epoch 7/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 762.3583 - mse: 762.3583 - mae: 15.2548 - rmse: 15.2548 - mape: 192.5355 - msle: 0.8763 - val_loss: 751.0182 - val_mse: 751.0182 - val_mae: 14.5578 - val_rmse: 14.5578 - val_mape: 175.0157 - val_msle: 0.7996\n",
      "Epoch 8/100\n",
      "2341/2341 [==============================] - 13s 5ms/step - loss: 753.4622 - mse: 753.4622 - mae: 15.1022 - rmse: 15.1022 - mape: 189.6794 - msle: 0.8655 - val_loss: 751.9312 - val_mse: 751.9312 - val_mae: 14.8902 - val_rmse: 14.8902 - val_mape: 181.8244 - val_msle: 0.8219\n",
      "Epoch 9/100\n",
      "2341/2341 [==============================] - 13s 6ms/step - loss: 752.5697 - mse: 752.5697 - mae: 15.1075 - rmse: 15.1075 - mape: 187.3329 - msle: 0.8580 - val_loss: 758.2093 - val_mse: 758.2093 - val_mae: 15.1687 - val_rmse: 15.1687 - val_mape: 201.8460 - val_msle: 0.8979\n",
      "Epoch 10/100\n",
      "2341/2341 [==============================] - 12s 5ms/step - loss: 743.4315 - mse: 743.4315 - mae: 14.9785 - rmse: 14.9785 - mape: 184.1043 - msle: 0.8438 - val_loss: 762.2936 - val_mse: 762.2936 - val_mae: 15.3149 - val_rmse: 15.3149 - val_mape: 183.2676 - val_msle: 0.8453\n",
      "Epoch 11/100\n",
      "2341/2341 [==============================] - 13s 5ms/step - loss: 744.8301 - mse: 744.8301 - mae: 14.9901 - rmse: 14.9901 - mape: 183.4452 - msle: 0.8412 - val_loss: 757.1705 - val_mse: 757.1705 - val_mae: 14.8921 - val_rmse: 14.8921 - val_mape: 182.4050 - val_msle: 0.8220\n",
      "Epoch 12/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 738.3174 - mse: 738.3174 - mae: 14.8930 - rmse: 14.8930 - mape: 186.0922 - msle: 0.8481 - val_loss: 757.3388 - val_mse: 757.3388 - val_mae: 14.4994 - val_rmse: 14.4994 - val_mape: 171.2133 - val_msle: 0.7895\n",
      "Epoch 13/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 733.9012 - mse: 733.9012 - mae: 14.8669 - rmse: 14.8669 - mape: 184.1244 - msle: 0.8409 - val_loss: 759.3834 - val_mse: 759.3834 - val_mae: 15.4422 - val_rmse: 15.4422 - val_mape: 209.8098 - val_msle: 0.9236\n",
      "Epoch 14/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 730.2950 - mse: 730.2950 - mae: 14.8002 - rmse: 14.8002 - mape: 175.7811 - msle: 0.8120 - val_loss: 757.8381 - val_mse: 757.8381 - val_mae: 14.0227 - val_rmse: 14.0227 - val_mape: 140.7657 - val_msle: 0.6940\n",
      "Epoch 15/100\n",
      "2341/2341 [==============================] - 16s 7ms/step - loss: 724.2541 - mse: 724.2541 - mae: 14.6909 - rmse: 14.6909 - mape: 175.9553 - msle: 0.8112 - val_loss: 754.4409 - val_mse: 754.4409 - val_mae: 14.6995 - val_rmse: 14.6995 - val_mape: 154.4525 - val_msle: 0.7350\n",
      "Epoch 16/100\n",
      "2341/2341 [==============================] - 13s 6ms/step - loss: 725.1420 - mse: 725.1420 - mae: 14.7285 - rmse: 14.7285 - mape: 181.9003 - msle: 0.8291 - val_loss: 752.1622 - val_mse: 752.1622 - val_mae: 14.7097 - val_rmse: 14.7097 - val_mape: 170.5513 - val_msle: 0.7857\n",
      "Epoch 17/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 719.9308 - mse: 719.9308 - mae: 14.6516 - rmse: 14.6516 - mape: 178.9778 - msle: 0.8172 - val_loss: 756.1675 - val_mse: 756.1675 - val_mae: 13.9320 - val_rmse: 13.9320 - val_mape: 127.2877 - val_msle: 0.6560\n",
      "Epoch 18/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 719.0283 - mse: 719.0283 - mae: 14.6284 - rmse: 14.6284 - mape: 176.2537 - msle: 0.8101 - val_loss: 756.1589 - val_mse: 756.1589 - val_mae: 14.2971 - val_rmse: 14.2971 - val_mape: 153.1340 - val_msle: 0.7276\n",
      "Epoch 19/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 713.6149 - mse: 713.6149 - mae: 14.5734 - rmse: 14.5734 - mape: 178.3010 - msle: 0.8152 - val_loss: 762.3181 - val_mse: 762.3181 - val_mae: 14.7143 - val_rmse: 14.7143 - val_mape: 173.8951 - val_msle: 0.7964\n",
      "Epoch 20/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 711.8097 - mse: 711.8097 - mae: 14.6045 - rmse: 14.6045 - mape: 177.7906 - msle: 0.8134 - val_loss: 758.5280 - val_mse: 758.5280 - val_mae: 15.0774 - val_rmse: 15.0774 - val_mape: 194.7549 - val_msle: 0.8648\n",
      "Epoch 21/100\n",
      "2341/2341 [==============================] - 15s 6ms/step - loss: 711.6761 - mse: 711.6761 - mae: 14.5359 - rmse: 14.5359 - mape: 173.1793 - msle: 0.7973 - val_loss: 754.2167 - val_mse: 754.2167 - val_mae: 14.7862 - val_rmse: 14.7862 - val_mape: 161.6382 - val_msle: 0.7584\n",
      "Epoch 22/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 704.0471 - mse: 704.0471 - mae: 14.4778 - rmse: 14.4778 - mape: 171.6629 - msle: 0.7919 - val_loss: 758.2225 - val_mse: 758.2225 - val_mae: 15.4506 - val_rmse: 15.4506 - val_mape: 201.5712 - val_msle: 0.8937\n",
      "Epoch 23/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 701.8677 - mse: 701.8677 - mae: 14.4434 - rmse: 14.4434 - mape: 175.5588 - msle: 0.8027 - val_loss: 755.4931 - val_mse: 755.4931 - val_mae: 14.8142 - val_rmse: 14.8142 - val_mape: 186.4137 - val_msle: 0.8358\n",
      "Epoch 24/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 702.9921 - mse: 702.9921 - mae: 14.4705 - rmse: 14.4705 - mape: 177.9049 - msle: 0.8120 - val_loss: 764.0775 - val_mse: 764.0775 - val_mae: 13.9973 - val_rmse: 13.9973 - val_mape: 137.5196 - val_msle: 0.6862\n",
      "Epoch 25/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 697.7956 - mse: 697.7956 - mae: 14.3730 - rmse: 14.3730 - mape: 178.2959 - msle: 0.8089 - val_loss: 765.8372 - val_mse: 765.8372 - val_mae: 14.2823 - val_rmse: 14.2823 - val_mape: 144.1595 - val_msle: 0.7071\n",
      "Epoch 26/100\n",
      "2341/2341 [==============================] - 15s 7ms/step - loss: 696.5411 - mse: 696.5411 - mae: 14.3553 - rmse: 14.3553 - mape: 174.1423 - msle: 0.7977 - val_loss: 762.5051 - val_mse: 762.5051 - val_mae: 15.0623 - val_rmse: 15.0623 - val_mape: 165.1058 - val_msle: 0.7734\n",
      "Epoch 27/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 691.1857 - mse: 691.1857 - mae: 14.3130 - rmse: 14.3130 - mape: 173.5464 - msle: 0.7938 - val_loss: 754.3936 - val_mse: 754.3936 - val_mae: 14.5679 - val_rmse: 14.5679 - val_mape: 169.1290 - val_msle: 0.7795\n",
      "Epoch 28/100\n",
      "2341/2341 [==============================] - 17s 7ms/step - loss: 682.9580 - mse: 682.9580 - mae: 14.1983 - rmse: 14.1983 - mape: 175.4506 - msle: 0.7960 - val_loss: 772.2318 - val_mse: 772.2318 - val_mae: 14.9824 - val_rmse: 14.9824 - val_mape: 182.4573 - val_msle: 0.8313\n",
      "Epoch 29/100\n",
      "2341/2341 [==============================] - 15s 6ms/step - loss: 680.8559 - mse: 680.8559 - mae: 14.1804 - rmse: 14.1804 - mape: 173.6793 - msle: 0.7924 - val_loss: 764.0378 - val_mse: 764.0378 - val_mae: 15.1503 - val_rmse: 15.1502 - val_mape: 202.5980 - val_msle: 0.8968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "2341/2341 [==============================] - 13s 6ms/step - loss: 680.1046 - mse: 680.1046 - mae: 14.1653 - rmse: 14.1653 - mape: 177.4539 - msle: 0.8035 - val_loss: 765.4863 - val_mse: 765.4863 - val_mae: 14.9709 - val_rmse: 14.9709 - val_mape: 175.4105 - val_msle: 0.8041\n",
      "Epoch 31/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 676.1561 - mse: 676.1561 - mae: 14.1022 - rmse: 14.1022 - mape: 175.0919 - msle: 0.7947 - val_loss: 767.3024 - val_mse: 767.3024 - val_mae: 14.3301 - val_rmse: 14.3301 - val_mape: 141.6830 - val_msle: 0.6995\n",
      "Epoch 32/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 673.3386 - mse: 673.3386 - mae: 14.0925 - rmse: 14.0925 - mape: 173.3364 - msle: 0.7876 - val_loss: 757.4036 - val_mse: 757.4036 - val_mae: 14.9711 - val_rmse: 14.9711 - val_mape: 179.1418 - val_msle: 0.8163\n",
      "Epoch 33/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 669.7418 - mse: 669.7418 - mae: 14.0351 - rmse: 14.0351 - mape: 170.5334 - msle: 0.7800 - val_loss: 781.2031 - val_mse: 781.2031 - val_mae: 14.8471 - val_rmse: 14.8471 - val_mape: 174.2006 - val_msle: 0.8020\n",
      "Epoch 34/100\n",
      "2341/2341 [==============================] - 15s 6ms/step - loss: 665.6434 - mse: 665.6434 - mae: 13.9884 - rmse: 13.9884 - mape: 171.4336 - msle: 0.7796 - val_loss: 767.3052 - val_mse: 767.3052 - val_mae: 14.9108 - val_rmse: 14.9108 - val_mape: 174.9973 - val_msle: 0.8021\n",
      "Epoch 35/100\n",
      "2341/2341 [==============================] - 15s 6ms/step - loss: 666.4383 - mse: 666.4383 - mae: 14.0112 - rmse: 14.0112 - mape: 171.1288 - msle: 0.7794 - val_loss: 777.7556 - val_mse: 777.7556 - val_mae: 14.9369 - val_rmse: 14.9369 - val_mape: 175.3067 - val_msle: 0.8060\n",
      "Epoch 36/100\n",
      "2341/2341 [==============================] - 16s 7ms/step - loss: 660.6851 - mse: 660.6851 - mae: 13.9483 - rmse: 13.9483 - mape: 172.1991 - msle: 0.7805 - val_loss: 770.7436 - val_mse: 770.7436 - val_mae: 14.0197 - val_rmse: 14.0197 - val_mape: 136.7903 - val_msle: 0.6817\n",
      "Epoch 37/100\n",
      "2341/2341 [==============================] - 15s 7ms/step - loss: 655.1824 - mse: 655.1824 - mae: 13.8817 - rmse: 13.8817 - mape: 168.4314 - msle: 0.7680 - val_loss: 784.2787 - val_mse: 784.2787 - val_mae: 14.9002 - val_rmse: 14.9002 - val_mape: 154.0425 - val_msle: 0.7411\n",
      "Epoch 38/100\n",
      "2341/2341 [==============================] - 16s 7ms/step - loss: 654.2786 - mse: 654.2786 - mae: 13.8897 - rmse: 13.8897 - mape: 171.7094 - msle: 0.7777 - val_loss: 791.9464 - val_mse: 791.9464 - val_mae: 15.0842 - val_rmse: 15.0842 - val_mape: 177.7721 - val_msle: 0.8161\n",
      "Epoch 39/100\n",
      "2341/2341 [==============================] - 15s 7ms/step - loss: 653.1541 - mse: 653.1541 - mae: 13.8680 - rmse: 13.8680 - mape: 171.9568 - msle: 0.7765 - val_loss: 776.6675 - val_mse: 776.6675 - val_mae: 14.9914 - val_rmse: 14.9914 - val_mape: 184.8526 - val_msle: 0.8397\n",
      "Epoch 40/100\n",
      "2341/2341 [==============================] - 15s 6ms/step - loss: 653.2964 - mse: 653.2964 - mae: 13.8734 - rmse: 13.8734 - mape: 168.8405 - msle: 0.7653 - val_loss: 771.8118 - val_mse: 771.8118 - val_mae: 14.3703 - val_rmse: 14.3703 - val_mape: 146.8526 - val_msle: 0.7149\n",
      "Epoch 41/100\n",
      "2341/2341 [==============================] - 15s 6ms/step - loss: 657.8441 - mse: 657.8441 - mae: 13.9002 - rmse: 13.9002 - mape: 170.5769 - msle: 0.7728 - val_loss: 778.3556 - val_mse: 778.3556 - val_mae: 15.2116 - val_rmse: 15.2116 - val_mape: 186.3382 - val_msle: 0.8419\n",
      "Epoch 42/100\n",
      "2341/2341 [==============================] - 16s 7ms/step - loss: 639.4135 - mse: 639.4135 - mae: 13.6723 - rmse: 13.6723 - mape: 167.9617 - msle: 0.7612 - val_loss: 792.2535 - val_mse: 792.2535 - val_mae: 14.5288 - val_rmse: 14.5288 - val_mape: 165.5028 - val_msle: 0.7753\n",
      "Epoch 43/100\n",
      "2341/2341 [==============================] - 16s 7ms/step - loss: 640.2753 - mse: 640.2753 - mae: 13.7150 - rmse: 13.7150 - mape: 169.5556 - msle: 0.7675 - val_loss: 789.5786 - val_mse: 789.5786 - val_mae: 14.6154 - val_rmse: 14.6154 - val_mape: 160.0338 - val_msle: 0.7564\n",
      "Epoch 44/100\n",
      "2341/2341 [==============================] - 16s 7ms/step - loss: 638.1397 - mse: 638.1397 - mae: 13.6356 - rmse: 13.6356 - mape: 168.4825 - msle: 0.7618 - val_loss: 785.8625 - val_mse: 785.8625 - val_mae: 15.2271 - val_rmse: 15.2271 - val_mape: 190.3127 - val_msle: 0.8583\n",
      "Epoch 45/100\n",
      "2341/2341 [==============================] - 15s 6ms/step - loss: 641.2512 - mse: 641.2512 - mae: 13.6964 - rmse: 13.6964 - mape: 170.9789 - msle: 0.7717 - val_loss: 783.4359 - val_mse: 783.4359 - val_mae: 13.8955 - val_rmse: 13.8955 - val_mape: 137.0181 - val_msle: 0.6874\n",
      "Epoch 46/100\n",
      "2341/2341 [==============================] - 15s 7ms/step - loss: 631.8325 - mse: 631.8325 - mae: 13.5714 - rmse: 13.5714 - mape: 168.0824 - msle: 0.7605 - val_loss: 794.6684 - val_mse: 794.6684 - val_mae: 14.8551 - val_rmse: 14.8551 - val_mape: 165.6236 - val_msle: 0.7755\n",
      "Epoch 47/100\n",
      "2341/2341 [==============================] - 15s 6ms/step - loss: 632.0709 - mse: 632.0709 - mae: 13.5599 - rmse: 13.5599 - mape: 168.1624 - msle: 0.7608 - val_loss: 785.3484 - val_mse: 785.3484 - val_mae: 15.1732 - val_rmse: 15.1732 - val_mape: 178.0376 - val_msle: 0.8198\n",
      "Epoch 48/100\n",
      "2341/2341 [==============================] - 16s 7ms/step - loss: 631.3048 - mse: 631.3048 - mae: 13.6013 - rmse: 13.6013 - mape: 168.6999 - msle: 0.7643 - val_loss: 783.5100 - val_mse: 783.5100 - val_mae: 14.1869 - val_rmse: 14.1869 - val_mape: 135.9914 - val_msle: 0.6847\n",
      "Epoch 49/100\n",
      "2341/2341 [==============================] - 16s 7ms/step - loss: 625.0911 - mse: 625.0911 - mae: 13.4756 - rmse: 13.4756 - mape: 169.8677 - msle: 0.7656 - val_loss: 788.2745 - val_mse: 788.2745 - val_mae: 14.3491 - val_rmse: 14.3491 - val_mape: 150.0244 - val_msle: 0.7239\n",
      "Epoch 50/100\n",
      "2341/2341 [==============================] - 16s 7ms/step - loss: 625.4844 - mse: 625.4844 - mae: 13.4773 - rmse: 13.4773 - mape: 170.8610 - msle: 0.7674 - val_loss: 783.4122 - val_mse: 783.4122 - val_mae: 14.6483 - val_rmse: 14.6483 - val_mape: 161.1313 - val_msle: 0.7617\n",
      "Epoch 51/100\n",
      "2341/2341 [==============================] - 15s 7ms/step - loss: 620.8675 - mse: 620.8675 - mae: 13.4478 - rmse: 13.4478 - mape: 167.3310 - msle: 0.7585 - val_loss: 793.3904 - val_mse: 793.3904 - val_mae: 14.7600 - val_rmse: 14.7600 - val_mape: 177.8393 - val_msle: 0.8143\n",
      "Epoch 52/100\n",
      "2341/2341 [==============================] - 15s 6ms/step - loss: 616.3181 - mse: 616.3181 - mae: 13.3946 - rmse: 13.3946 - mape: 167.6109 - msle: 0.7564 - val_loss: 789.3052 - val_mse: 789.3052 - val_mae: 14.8955 - val_rmse: 14.8955 - val_mape: 179.2140 - val_msle: 0.8213\n",
      "Epoch 53/100\n",
      "2341/2341 [==============================] - 15s 7ms/step - loss: 616.5866 - mse: 616.5866 - mae: 13.3853 - rmse: 13.3853 - mape: 169.3926 - msle: 0.7641 - val_loss: 789.0980 - val_mse: 789.0980 - val_mae: 14.3691 - val_rmse: 14.3691 - val_mape: 161.0569 - val_msle: 0.7596\n",
      "Epoch 54/100\n",
      "2341/2341 [==============================] - 16s 7ms/step - loss: 611.0638 - mse: 611.0638 - mae: 13.3573 - rmse: 13.3573 - mape: 169.0638 - msle: 0.7598 - val_loss: 790.3281 - val_mse: 790.3281 - val_mae: 14.8189 - val_rmse: 14.8189 - val_mape: 162.5873 - val_msle: 0.7675\n",
      "Epoch 55/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 614.7634 - mse: 614.7634 - mae: 13.3871 - rmse: 13.3871 - mape: 167.8121 - msle: 0.7570 - val_loss: 795.6703 - val_mse: 795.6703 - val_mae: 15.5667 - val_rmse: 15.5667 - val_mape: 204.9227 - val_msle: 0.9149\n",
      "Epoch 56/100\n",
      "2341/2341 [==============================] - 14s 6ms/step - loss: 612.6959 - mse: 612.6959 - mae: 13.3688 - rmse: 13.3688 - mape: 170.8706 - msle: 0.7672 - val_loss: 812.9818 - val_mse: 812.9818 - val_mae: 14.4623 - val_rmse: 14.4623 - val_mape: 155.3953 - val_msle: 0.7452\n",
      "Epoch 57/100\n",
      "2341/2341 [==============================] - 16s 7ms/step - loss: 612.1371 - mse: 612.1371 - mae: 13.3699 - rmse: 13.3699 - mape: 170.4706 - msle: 0.7650 - val_loss: 804.1050 - val_mse: 804.1050 - val_mae: 14.1715 - val_rmse: 14.1715 - val_mape: 142.9722 - val_msle: 0.7081\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2341/2341 [==============================] - 17s 7ms/step - loss: 609.7729 - mse: 609.7729 - mae: 13.3436 - rmse: 13.3436 - mape: 169.1893 - msle: 0.7625 - val_loss: 803.0488 - val_mse: 803.0488 - val_mae: 14.6795 - val_rmse: 14.6795 - val_mape: 158.8362 - val_msle: 0.7567\n",
      "Epoch 59/100\n",
      " 410/2341 [====>.........................] - ETA: 11s - loss: 611.6042 - mse: 611.6042 - mae: 13.4027 - rmse: 13.4027 - mape: 167.7480 - msle: 0.7608"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 100 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train_reg,validation_split=0.33, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['mse'], color = '#4472C4')\n",
    "plt.plot(history.history['val_mse'], color = '#ED7D31')\n",
    "plt.title('Model MSE')\n",
    "plt.ylabel('MSE')\n",
    "#plt.ylim(0.7,0.98)\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('results/DNN_MSE_Reg.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_per_epoch = history.history['val_mse']\n",
    "best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "print(str(model),\"--- %s seconds ---\" % ((time.time() - start_time)))\n",
    "\n",
    "# Retrain the model\n",
    "history = hypermodel.fit(X_train, y_train_reg,validation_split=0.33, epochs=best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print Model Layout:\n",
    "plot_model(hypermodel, to_file='hypermodel_reg.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = hypermodel.evaluate(X_test, y_test_reg)\n",
    "print(\"[loss, MSE, MAE, RMSE, MAPE, MSLE]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "loss_per_fold = []\n",
    "MAE_per_fold = []\n",
    "\n",
    "inputs = np.concatenate((X_train, X_test), axis=0)\n",
    "targets = np.concatenate((y_train_reg, y_test_reg), axis=0)\n",
    "\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    best_model.fit(inputs[train], targets[train], epochs=best_epoch, verbose=0, callbacks = [stop_early])\n",
    "    score = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {score}')\n",
    "\n",
    "    yhat = model.predict(inputs[test])\n",
    "    print('MAE: %.3f' % mean_absolute_error(targets[test], yhat))\n",
    "\n",
    "    MAE_per_fold.append(mean_absolute_error(targets[test], yhat))\n",
    "    loss_per_fold.append(score)\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "mse = []\n",
    "mae = []\n",
    "rmse = []\n",
    "mape = []\n",
    "msle = []\n",
    "for i in range(len(loss_per_fold)):\n",
    "    loss.append(loss_per_fold[i][0])\n",
    "    mse.append(loss_per_fold[i][1])\n",
    "    mae.append(loss_per_fold[i][2])\n",
    "    rmse.append(loss_per_fold[i][3])\n",
    "    mape.append(loss_per_fold[i][4])\n",
    "    msle.append(loss_per_fold[i][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Loss: %s\" % statistics.mean(loss))\n",
    "print(\"Mean MSE: %s\" % statistics.mean(mse))\n",
    "print(\"Mean MAE: %s\" % statistics.mean(mae))\n",
    "print(\"Mean RMSE: %s\" % statistics.mean(rmse))\n",
    "print(\"Mean MAPE: %s\" % statistics.mean(mape))\n",
    "print(\"Mean MSLE: %s\" % statistics.mean(msle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SD Loss: %s\" % statistics.stdev(loss))\n",
    "print(\"SD MSE: %s\" % statistics.stdev(mse))\n",
    "print(\"SD MAE: %s\" % statistics.stdev(mae))\n",
    "print(\"SD RMSE: %s\" % statistics.stdev(rmse))\n",
    "print(\"SD MAPE: %s\" % statistics.stdev(mape))\n",
    "print(\"SD MSLE: %s\" % statistics.stdev(msle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "#Create the plots\n",
    "plt.plot(range(1,len(loss)+1),loss, color = '#4472C4', label = 'MSE')\n",
    "#plt.plot(range(1,len(mape)+1),mape, color='red', label = 'MAPE')\n",
    "#Create the title, axis description and legend\n",
    "#plt.title('Results KFOLD Cross-Validation')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel(\"Numer of Fold\")\n",
    "#plt.legend()\n",
    "#plt.grid()\n",
    "plt.xticks(rotation = 0)\n",
    "plt.savefig('results/DNN_KFold_MSE.png')\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "#Create the plots\n",
    "#plt.plot(range(1,len(loss)+1),loss, color='blue', label = 'MSE')\n",
    "plt.plot(range(1,len(mape)+1),mape, color = '#4472C4', label = 'MAPE')\n",
    "#Create the title, axis description and legend\n",
    "#plt.title('Results KFOLD Cross-Validation')\n",
    "plt.ylabel('MAPE')\n",
    "plt.xlabel(\"Numer of Fold\")\n",
    "#plt.legend()\n",
    "#plt.grid()\n",
    "plt.xticks(rotation = 0)\n",
    "plt.savefig('results/DNN_KFold_MAPE.png')\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "#Create the plots\n",
    "#plt.plot(range(1,len(MAE_per_fold)+1),MAE_per_fold, color='blue', label = 'MAE')\n",
    "plt.plot(range(1,len(msle)+1),msle, color = '#4472C4', label = 'MSLE')\n",
    "#Create the title, axis description and legend\n",
    "#plt.title('Results KFOLD Cross-Validation')\n",
    "plt.ylabel('MSLE')\n",
    "plt.xlabel(\"Numer of Fold\")\n",
    "#plt.legend()\n",
    "#plt.grid()\n",
    "plt.xticks(rotation = 0)\n",
    "plt.savefig('results/DNN_KFold_MSLE.png')\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
