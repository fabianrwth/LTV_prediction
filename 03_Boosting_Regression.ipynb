{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# library options\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../data/final/X_train.parquet')\n",
    "X_test = pd.read_parquet('../data/final/X_test.parquet')\n",
    "                      \n",
    "y_test_reg = pd.read_parquet('../data/final/y_test.parquet')['sum_payments_package_key_ltv']\n",
    "y_train_reg = pd.read_parquet('../data/final/y_train.parquet')['sum_payments_package_key_ltv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Settings for K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# with transactional features\n",
    "inputs = np.concatenate((X_train, X_test), axis=0)\n",
    "targets = np.concatenate((y_train_reg, y_test_reg), axis=0)\n",
    "\n",
    "# withhout transactional features\n",
    "X_train_0 = X_train.drop(['sum_day_1','sum_day_2','sum_day_3','gradient','clumpiness'], axis=1).values\n",
    "X_test_0 = X_test.drop(['sum_day_1','sum_day_2','sum_day_3','gradient','clumpiness'], axis=1).values\n",
    "\n",
    "inputs_0 = np.concatenate((X_train_0, X_test_0), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "MSE ---- mean:  970.3835535036854 --- std:  25.497907898302227\n",
      "MAE ---- mean:  23.312037007111385 --- std:  0.4103034249033027\n",
      "MSLE ---- mean:  1.8347464919283898 --- std:  0.04850862045657901\n"
     ]
    }
   ],
   "source": [
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "# Initalize empty scores list\n",
    "scores = np.zeros(shape=(3,num_folds))\n",
    "\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Chosing Model\n",
    "    model = AdaBoostRegressor()\n",
    "    model.fit(inputs[train], targets[train])\n",
    "\n",
    "    y_predict = model.predict(inputs[test])\n",
    "    y_predict_clipped = y_predict.clip(min=0) # setting min value to zero  (requirement for log_error)\n",
    "\n",
    "    # filling scores list with the chosen sco\n",
    "    scores[0,fold_no -1] = metrics.mean_squared_error(targets[test], y_predict)\n",
    "    scores[1,fold_no -1] = metrics.mean_absolute_error(targets[test], y_predict)\n",
    "    scores[2,fold_no -1] = metrics.mean_squared_log_error(targets[test], y_predict_clipped) # cannot handle negative input values as y_predict\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "scores_ada = scores\n",
    "\n",
    "print('MSE ---- mean: ',scores_ada[0].mean(), '--- std: ',scores_ada[0].std())\n",
    "print('MAE ---- mean: ',scores_ada[1].mean(), '--- std: ',scores_ada[1].std())\n",
    "print('MSLE ---- mean: ',scores_ada[2].mean(), '--- std: ',scores_ada[2].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without transactional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "--------------WITHOUT TRANSACTIONAL FEATURES-----------\n",
      "MSE ---- mean:  1000.7282982893639 --- std:  18.14036388301761\n",
      "MAE ---- mean:  23.874959058314946 --- std:  0.22537868978650996\n",
      "MSLE ---- mean:  1.8815224842335638 --- std:  0.04910654749391794\n"
     ]
    }
   ],
   "source": [
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "# Initalize empty scores list\n",
    "scores = np.zeros(shape=(3,num_folds))\n",
    "\n",
    "for train, test in kfold.split(inputs_0, targets):\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Chosing Model\n",
    "    model = AdaBoostRegressor()\n",
    "    model.fit(inputs_0[train], targets[train])\n",
    "\n",
    "    y_predict = model.predict(inputs_0[test])\n",
    "    y_predict_clipped = y_predict.clip(min=0) # setting min value to zero  (requirement for log_error)\n",
    "\n",
    "    # filling scores list with the chosen sco\n",
    "    scores[0,fold_no -1] = metrics.mean_squared_error(targets[test], y_predict)\n",
    "    scores[1,fold_no -1] = metrics.mean_absolute_error(targets[test], y_predict)\n",
    "    scores[2,fold_no -1] = metrics.mean_squared_log_error(targets[test], y_predict_clipped) # cannot handle negative input values as y_predict\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "scores_ada = scores\n",
    "print('--------------WITHOUT TRANSACTIONAL FEATURES-----------')\n",
    "print('MSE ---- mean: ',scores_ada[0].mean(), '--- std: ',scores_ada[0].std())\n",
    "print('MAE ---- mean: ',scores_ada[1].mean(), '--- std: ',scores_ada[1].std())\n",
    "print('MSLE ---- mean: ',scores_ada[2].mean(), '--- std: ',scores_ada[2].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "MSE ---- mean:  731.9468281305169 --- std:  27.23287081699981\n",
      "MAE ---- mean:  14.699817516811711 --- std:  0.22413085165571323\n",
      "MSLE ---- mean:  0.7231586644824353 --- std:  0.01014176859452283\n"
     ]
    }
   ],
   "source": [
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "# Initalize empty scores list\n",
    "scores = np.zeros(shape=(3,num_folds))\n",
    "\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Chosing Model\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(inputs[train], targets[train])\n",
    "\n",
    "    y_predict = model.predict(inputs[test])\n",
    "    y_predict_clipped = y_predict.clip(min=0) # setting min value to zero  (requirement for log_error)\n",
    "\n",
    "    # filling scores list with the chosen sco\n",
    "    scores[0,fold_no -1] = metrics.mean_squared_error(targets[test], y_predict)\n",
    "    scores[1,fold_no -1] = metrics.mean_absolute_error(targets[test], y_predict)\n",
    "    scores[2,fold_no -1] = metrics.mean_squared_log_error(targets[test], y_predict_clipped) # cannot handle negative input values as y_predict\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "scores_gb = scores\n",
    "\n",
    "print('MSE ---- mean: ',scores_gb[0].mean(), '--- std: ',scores_gb[0].std())\n",
    "print('MAE ---- mean: ',scores_gb[1].mean(), '--- std: ',scores_gb[1].std())\n",
    "print('MSLE ---- mean: ',scores_gb[2].mean(), '--- std: ',scores_gb[2].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without transactional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "--------------WITHOUT TRANSACTIONAL FEATURES-----------\n",
      "MSE ---- mean:  750.3103053163693 --- std:  16.789982779582424\n",
      "MAE ---- mean:  15.054962821149633 --- std:  0.1390222079705866\n",
      "MSLE ---- mean:  0.7377859759616954 --- std:  0.008133110736820482\n"
     ]
    }
   ],
   "source": [
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "# Initalize empty scores list\n",
    "scores = np.zeros(shape=(3,num_folds))\n",
    "\n",
    "for train, test in kfold.split(inputs_0, targets):\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Chosing Model\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(inputs_0[train], targets[train])\n",
    "\n",
    "    y_predict = model.predict(inputs_0[test])\n",
    "    y_predict_clipped = y_predict.clip(min=0) # setting min value to zero  (requirement for log_error)\n",
    "\n",
    "    # filling scores list with the chosen sco\n",
    "    scores[0,fold_no -1] = metrics.mean_squared_error(targets[test], y_predict)\n",
    "    scores[1,fold_no -1] = metrics.mean_absolute_error(targets[test], y_predict)\n",
    "    scores[2,fold_no -1] = metrics.mean_squared_log_error(targets[test], y_predict_clipped) # cannot handle negative input values as y_predict\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "scores_gb = scores\n",
    "print('--------------WITHOUT TRANSACTIONAL FEATURES-----------')\n",
    "print('MSE ---- mean: ',scores_gb[0].mean(), '--- std: ',scores_gb[0].std())\n",
    "print('MAE ---- mean: ',scores_gb[1].mean(), '--- std: ',scores_gb[1].std())\n",
    "print('MSLE ---- mean: ',scores_gb[2].mean(), '--- std: ',scores_gb[2].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning with RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I. Rough Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.6, learning_rate=0.3, max_depth=6, n_estimators=150, reg_alpha=1, reg_lambda=0, subsample=0.7999999999999999; total time=  32.0s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.6, learning_rate=0.3, max_depth=6, n_estimators=150, reg_alpha=1, reg_lambda=0, subsample=0.7999999999999999; total time=  35.5s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.6, learning_rate=0.3, max_depth=6, n_estimators=150, reg_alpha=1, reg_lambda=0, subsample=0.7999999999999999; total time=  35.1s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.6, learning_rate=0.3, max_depth=6, n_estimators=150, reg_alpha=1, reg_lambda=0, subsample=0.7999999999999999; total time=  35.0s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.6, learning_rate=0.01, max_depth=4, n_estimators=200, reg_alpha=1, reg_lambda=0, subsample=0.6; total time=  34.2s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.6, learning_rate=0.01, max_depth=4, n_estimators=200, reg_alpha=1, reg_lambda=0, subsample=0.6; total time=  33.6s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.6, learning_rate=0.01, max_depth=4, n_estimators=200, reg_alpha=1, reg_lambda=0, subsample=0.6; total time=  33.3s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.6, learning_rate=0.01, max_depth=4, n_estimators=200, reg_alpha=1, reg_lambda=0, subsample=0.6; total time=  34.7s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.5, learning_rate=0.01, max_depth=2, n_estimators=100, reg_alpha=0.5, reg_lambda=0.5, subsample=0.6; total time=  14.8s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.5, learning_rate=0.01, max_depth=2, n_estimators=100, reg_alpha=0.5, reg_lambda=0.5, subsample=0.6; total time=  15.3s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.5, learning_rate=0.01, max_depth=2, n_estimators=100, reg_alpha=0.5, reg_lambda=0.5, subsample=0.6; total time=  14.9s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.5, learning_rate=0.01, max_depth=2, n_estimators=100, reg_alpha=0.5, reg_lambda=0.5, subsample=0.6; total time=  14.6s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.01, max_depth=2, n_estimators=150, reg_alpha=0.5, reg_lambda=1, subsample=0.8999999999999999; total time=  23.6s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.01, max_depth=2, n_estimators=150, reg_alpha=0.5, reg_lambda=1, subsample=0.8999999999999999; total time=  23.2s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.01, max_depth=2, n_estimators=150, reg_alpha=0.5, reg_lambda=1, subsample=0.8999999999999999; total time=  23.1s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.01, max_depth=2, n_estimators=150, reg_alpha=0.5, reg_lambda=1, subsample=0.8999999999999999; total time=  23.3s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.5, learning_rate=0.01, max_depth=8, n_estimators=150, reg_alpha=0.5, reg_lambda=0, subsample=0.5; total time= 1.1min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.5, learning_rate=0.01, max_depth=8, n_estimators=150, reg_alpha=0.5, reg_lambda=0, subsample=0.5; total time= 1.1min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.5, learning_rate=0.01, max_depth=8, n_estimators=150, reg_alpha=0.5, reg_lambda=0, subsample=0.5; total time=  52.2s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.5, learning_rate=0.01, max_depth=8, n_estimators=150, reg_alpha=0.5, reg_lambda=0, subsample=0.5; total time=  50.9s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=50, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7; total time=   7.5s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=50, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7; total time=   7.5s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=50, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7; total time=   7.3s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=50, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7; total time=   7.5s\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.5, learning_rate=0.2, max_depth=2, n_estimators=50, reg_alpha=1, reg_lambda=1, subsample=0.6; total time=   4.6s\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.5, learning_rate=0.2, max_depth=2, n_estimators=50, reg_alpha=1, reg_lambda=1, subsample=0.6; total time=   4.6s\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.5, learning_rate=0.2, max_depth=2, n_estimators=50, reg_alpha=1, reg_lambda=1, subsample=0.6; total time=   4.8s\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.5, learning_rate=0.2, max_depth=2, n_estimators=50, reg_alpha=1, reg_lambda=1, subsample=0.6; total time=   4.7s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=50, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   6.6s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=50, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   6.5s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=50, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   6.7s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=50, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   6.3s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.6, learning_rate=0.1, max_depth=4, n_estimators=50, reg_alpha=0.5, reg_lambda=0.5, subsample=0.6; total time=   9.4s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.6, learning_rate=0.1, max_depth=4, n_estimators=50, reg_alpha=0.5, reg_lambda=0.5, subsample=0.6; total time=   9.3s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.6, learning_rate=0.1, max_depth=4, n_estimators=50, reg_alpha=0.5, reg_lambda=0.5, subsample=0.6; total time=   9.3s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.6, learning_rate=0.1, max_depth=4, n_estimators=50, reg_alpha=0.5, reg_lambda=0.5, subsample=0.6; total time=   9.4s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.6, learning_rate=0.2, max_depth=2, n_estimators=150, reg_alpha=1, reg_lambda=0, subsample=0.6; total time=  20.4s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.6, learning_rate=0.2, max_depth=2, n_estimators=150, reg_alpha=1, reg_lambda=0, subsample=0.6; total time=  21.9s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.6, learning_rate=0.2, max_depth=2, n_estimators=150, reg_alpha=1, reg_lambda=0, subsample=0.6; total time=  22.4s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.6, learning_rate=0.2, max_depth=2, n_estimators=150, reg_alpha=1, reg_lambda=0, subsample=0.6; total time=  27.5s\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.4, learning_rate=0.1, max_depth=3, n_estimators=150, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=  17.5s\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.4, learning_rate=0.1, max_depth=3, n_estimators=150, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=  17.7s\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.4, learning_rate=0.1, max_depth=3, n_estimators=150, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=  21.1s\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.4, learning_rate=0.1, max_depth=3, n_estimators=150, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=  20.4s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, reg_alpha=1, reg_lambda=0.5, subsample=0.8999999999999999; total time=  26.9s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, reg_alpha=1, reg_lambda=0.5, subsample=0.8999999999999999; total time=  26.0s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, reg_alpha=1, reg_lambda=0.5, subsample=0.8999999999999999; total time=  20.2s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, reg_alpha=1, reg_lambda=0.5, subsample=0.8999999999999999; total time=  24.2s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.4, learning_rate=0.01, max_depth=2, n_estimators=200, reg_alpha=0, reg_lambda=0, subsample=0.7; total time=  17.9s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.4, learning_rate=0.01, max_depth=2, n_estimators=200, reg_alpha=0, reg_lambda=0, subsample=0.7; total time=  18.1s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.4, learning_rate=0.01, max_depth=2, n_estimators=200, reg_alpha=0, reg_lambda=0, subsample=0.7; total time=  17.5s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.4, learning_rate=0.01, max_depth=2, n_estimators=200, reg_alpha=0, reg_lambda=0, subsample=0.7; total time=  17.6s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=150, reg_alpha=0, reg_lambda=0.5, subsample=0.7999999999999999; total time=  31.8s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=150, reg_alpha=0, reg_lambda=0.5, subsample=0.7999999999999999; total time=  31.8s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=150, reg_alpha=0, reg_lambda=0.5, subsample=0.7999999999999999; total time=  32.6s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=150, reg_alpha=0, reg_lambda=0.5, subsample=0.7999999999999999; total time=  32.1s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=3, n_estimators=150, reg_alpha=1, reg_lambda=0.5, subsample=0.6; total time=  33.5s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=3, n_estimators=150, reg_alpha=1, reg_lambda=0.5, subsample=0.6; total time=  33.6s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=3, n_estimators=150, reg_alpha=1, reg_lambda=0.5, subsample=0.6; total time=  35.0s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=3, n_estimators=150, reg_alpha=1, reg_lambda=0.5, subsample=0.6; total time=  37.8s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.6, learning_rate=0.3, max_depth=5, n_estimators=150, reg_alpha=1, reg_lambda=0.5, subsample=0.5; total time=  45.5s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.6, learning_rate=0.3, max_depth=5, n_estimators=150, reg_alpha=1, reg_lambda=0.5, subsample=0.5; total time=  40.2s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.6, learning_rate=0.3, max_depth=5, n_estimators=150, reg_alpha=1, reg_lambda=0.5, subsample=0.5; total time=  34.8s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.6, learning_rate=0.3, max_depth=5, n_estimators=150, reg_alpha=1, reg_lambda=0.5, subsample=0.5; total time=  39.4s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.5, learning_rate=0.2, max_depth=8, n_estimators=200, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7; total time= 1.6min\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.5, learning_rate=0.2, max_depth=8, n_estimators=200, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7; total time= 1.3min\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.5, learning_rate=0.2, max_depth=8, n_estimators=200, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7; total time= 1.3min\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.5, learning_rate=0.2, max_depth=8, n_estimators=200, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7; total time= 1.2min\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.2, max_depth=8, n_estimators=100, reg_alpha=0, reg_lambda=0, subsample=0.7999999999999999; total time=  43.2s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.2, max_depth=8, n_estimators=100, reg_alpha=0, reg_lambda=0, subsample=0.7999999999999999; total time=  43.7s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.2, max_depth=8, n_estimators=100, reg_alpha=0, reg_lambda=0, subsample=0.7999999999999999; total time=  43.1s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.2, max_depth=8, n_estimators=100, reg_alpha=0, reg_lambda=0, subsample=0.7999999999999999; total time=  58.3s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.4, learning_rate=0.2, max_depth=8, n_estimators=100, reg_alpha=1, reg_lambda=1, subsample=0.5; total time=  28.5s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.4, learning_rate=0.2, max_depth=8, n_estimators=100, reg_alpha=1, reg_lambda=1, subsample=0.5; total time=  27.2s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.4, learning_rate=0.2, max_depth=8, n_estimators=100, reg_alpha=1, reg_lambda=1, subsample=0.5; total time=  26.7s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.4, learning_rate=0.2, max_depth=8, n_estimators=100, reg_alpha=1, reg_lambda=1, subsample=0.5; total time=  28.1s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=100, reg_alpha=0.5, reg_lambda=0.5, subsample=0.5; total time=  14.2s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=100, reg_alpha=0.5, reg_lambda=0.5, subsample=0.5; total time=  20.4s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=100, reg_alpha=0.5, reg_lambda=0.5, subsample=0.5; total time=  14.7s\n",
      "[CV] END colsample_bylevel=0.5, colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=100, reg_alpha=0.5, reg_lambda=0.5, subsample=0.5; total time=  15.8s\n",
      "Best parameters: {'subsample': 0.6, 'reg_lambda': 0.5, 'reg_alpha': 1, 'n_estimators': 150, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8999999999999999, 'colsample_bylevel': 0.7999999999999999}\n",
      "Lowest RMSE:  726.7421046670233\n"
     ]
    }
   ],
   "source": [
    "params = { 'max_depth': [2,3,4,5,6,8],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "    'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "    'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'reg_alpha': [0, 0.5, 1],\n",
    "    'reg_lambda': [0, 0.5, 1]}\n",
    "    \n",
    "xgbr = xgb.XGBRegressor()\n",
    "    \n",
    "xgb_search = RandomizedSearchCV(estimator=xgbr,\n",
    "                    cv=4,\n",
    "                    param_distributions=params,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    n_iter=20,\n",
    "                    verbose=2)\n",
    "                    \n",
    "xgb_search.fit(X_train.values, y_train_reg.values)\n",
    "                    \n",
    "print(\"Best parameters:\", xgb_search.best_params_)\n",
    "print(\"Lowest RMSE: \", (-xgb_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_colsample_bylevel</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_reg_alpha</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>150</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-726.742105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-730.858411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-732.018837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-732.332168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-732.453772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-733.010468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-748.639804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>150</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-761.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-780.875134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-787.137057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-788.279544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-794.113786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-797.576673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-801.520597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-823.726123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-865.293198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-901.733499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1019.987464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1065.779063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-1417.771750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_n_estimators param_learning_rate param_max_depth  \\\n",
       "14                150                 0.1               3   \n",
       "9                 150                 0.2               2   \n",
       "5                  50                 0.3               3   \n",
       "10                150                 0.1               3   \n",
       "19                100                 0.1               3   \n",
       "8                  50                 0.1               4   \n",
       "6                  50                 0.2               2   \n",
       "13                150                 0.3               5   \n",
       "17                100                 0.2               8   \n",
       "18                100                 0.2               8   \n",
       "15                150                 0.3               5   \n",
       "1                 200                0.01               4   \n",
       "0                 150                 0.3               6   \n",
       "16                200                 0.2               8   \n",
       "4                 150                0.01               8   \n",
       "12                200                0.01               2   \n",
       "3                 150                0.01               2   \n",
       "11                100                0.01               3   \n",
       "2                 100                0.01               2   \n",
       "7                  50                0.01               3   \n",
       "\n",
       "   param_colsample_bytree param_colsample_bylevel param_subsample  \\\n",
       "14                    0.9                     0.8             0.6   \n",
       "9                     0.6                     0.9             0.6   \n",
       "5                     0.5                     0.9             0.7   \n",
       "10                    0.4                     0.4             0.6   \n",
       "19                    0.5                     0.5             0.5   \n",
       "8                     0.6                     0.8             0.6   \n",
       "6                     0.5                     0.4             0.6   \n",
       "13                    0.5                     0.7             0.8   \n",
       "17                    0.8                     0.6             0.8   \n",
       "18                    0.4                     0.5             0.5   \n",
       "15                    0.6                     0.7             0.5   \n",
       "1                     0.6                     0.5             0.6   \n",
       "0                     0.6                     0.5             0.8   \n",
       "16                    0.5                     0.8             0.7   \n",
       "4                     0.5                     0.9             0.5   \n",
       "12                    0.4                     0.5             0.7   \n",
       "3                     0.7                     0.9             0.9   \n",
       "11                    0.7                     0.9             0.9   \n",
       "2                     0.5                     0.9             0.6   \n",
       "7                     0.5                     0.7             0.7   \n",
       "\n",
       "   param_reg_alpha param_reg_lambda  mean_test_score  \n",
       "14               1              0.5      -726.742105  \n",
       "9                1                0      -730.858411  \n",
       "5              0.5              0.5      -732.018837  \n",
       "10               0                1      -732.332168  \n",
       "19             0.5              0.5      -732.453772  \n",
       "8              0.5              0.5      -733.010468  \n",
       "6                1                1      -748.639804  \n",
       "13               0              0.5      -761.149100  \n",
       "17               0                0      -780.875134  \n",
       "18               1                1      -787.137057  \n",
       "15               1              0.5      -788.279544  \n",
       "1                1                0      -794.113786  \n",
       "0                1                0      -797.576673  \n",
       "16             0.5              0.5      -801.520597  \n",
       "4              0.5                0      -823.726123  \n",
       "12               0                0      -865.293198  \n",
       "3              0.5                1      -901.733499  \n",
       "11               1              0.5     -1019.987464  \n",
       "2              0.5              0.5     -1065.779063  \n",
       "7              0.5                1     -1417.771750  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(xgb_search.cv_results_)\n",
    "\n",
    "df[['param_n_estimators',\n",
    "'param_learning_rate',\n",
    "'param_max_depth', \n",
    "'param_colsample_bytree', \n",
    "'param_colsample_bylevel',\n",
    "'param_subsample',\n",
    "'param_reg_alpha',\n",
    "'param_reg_lambda',\n",
    "'mean_test_score']].sort_values(by ='mean_test_score', ascending=False)[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### II. Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.9999999999999999, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=1.2000000000000002, reg_lambda=1.0, subsample=0.6; total time=  20.7s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.9999999999999999, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=1.2000000000000002, reg_lambda=1.0, subsample=0.6; total time=  24.0s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.9999999999999999, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=1.2000000000000002, reg_lambda=1.0, subsample=0.6; total time=  23.3s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.9999999999999999, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=1.2000000000000002, reg_lambda=1.0, subsample=0.6; total time=  25.5s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.05, max_depth=3, n_estimators=125, reg_alpha=1.0, reg_lambda=1.0, subsample=0.6; total time=  39.3s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.05, max_depth=3, n_estimators=125, reg_alpha=1.0, reg_lambda=1.0, subsample=0.6; total time=  43.4s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.05, max_depth=3, n_estimators=125, reg_alpha=1.0, reg_lambda=1.0, subsample=0.6; total time=  35.3s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.05, max_depth=3, n_estimators=125, reg_alpha=1.0, reg_lambda=1.0, subsample=0.6; total time=  32.8s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=2, n_estimators=175, reg_alpha=1.0, reg_lambda=0.0, subsample=0.6; total time=  24.2s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=2, n_estimators=175, reg_alpha=1.0, reg_lambda=0.0, subsample=0.6; total time=  24.0s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=2, n_estimators=175, reg_alpha=1.0, reg_lambda=0.0, subsample=0.6; total time=  27.4s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=2, n_estimators=175, reg_alpha=1.0, reg_lambda=0.0, subsample=0.6; total time=  26.6s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7999999999999999, learning_rate=0.2, max_depth=3, n_estimators=175, reg_alpha=0.6000000000000001, reg_lambda=1.0, subsample=0.6; total time=  42.2s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7999999999999999, learning_rate=0.2, max_depth=3, n_estimators=175, reg_alpha=0.6000000000000001, reg_lambda=1.0, subsample=0.6; total time=  42.8s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7999999999999999, learning_rate=0.2, max_depth=3, n_estimators=175, reg_alpha=0.6000000000000001, reg_lambda=1.0, subsample=0.6; total time=  32.6s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7999999999999999, learning_rate=0.2, max_depth=3, n_estimators=175, reg_alpha=0.6000000000000001, reg_lambda=1.0, subsample=0.6; total time=  34.0s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=150, reg_alpha=0.6000000000000001, reg_lambda=0.8, subsample=0.5; total time=  27.4s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=150, reg_alpha=0.6000000000000001, reg_lambda=0.8, subsample=0.5; total time=  27.1s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=150, reg_alpha=0.6000000000000001, reg_lambda=0.8, subsample=0.5; total time=  28.0s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=150, reg_alpha=0.6000000000000001, reg_lambda=0.8, subsample=0.5; total time=  25.6s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=150, reg_alpha=0.6000000000000001, reg_lambda=0.0, subsample=0.5; total time=  33.0s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=150, reg_alpha=0.6000000000000001, reg_lambda=0.0, subsample=0.5; total time=  35.5s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=150, reg_alpha=0.6000000000000001, reg_lambda=0.0, subsample=0.5; total time=  35.2s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=150, reg_alpha=0.6000000000000001, reg_lambda=0.0, subsample=0.5; total time=  36.8s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.2, max_depth=3, n_estimators=175, reg_alpha=0.8, reg_lambda=1.0, subsample=0.5; total time=  43.5s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.2, max_depth=3, n_estimators=175, reg_alpha=0.8, reg_lambda=1.0, subsample=0.5; total time=  45.2s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.2, max_depth=3, n_estimators=175, reg_alpha=0.8, reg_lambda=1.0, subsample=0.5; total time=  43.8s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.2, max_depth=3, n_estimators=175, reg_alpha=0.8, reg_lambda=1.0, subsample=0.5; total time=  43.3s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.2, max_depth=3, n_estimators=175, reg_alpha=0.0, reg_lambda=0.6000000000000001, subsample=0.6; total time=  31.7s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.2, max_depth=3, n_estimators=175, reg_alpha=0.0, reg_lambda=0.6000000000000001, subsample=0.6; total time=  31.4s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.2, max_depth=3, n_estimators=175, reg_alpha=0.0, reg_lambda=0.6000000000000001, subsample=0.6; total time=  31.2s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.2, max_depth=3, n_estimators=175, reg_alpha=0.0, reg_lambda=0.6000000000000001, subsample=0.6; total time=  36.2s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.6; total time=  24.6s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.6; total time=  25.5s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.6; total time=  24.8s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.6; total time=  23.3s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=0.0, reg_lambda=0.6000000000000001, subsample=0.5; total time=  20.3s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=0.0, reg_lambda=0.6000000000000001, subsample=0.5; total time=  25.9s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=0.0, reg_lambda=0.6000000000000001, subsample=0.5; total time=  28.9s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=0.0, reg_lambda=0.6000000000000001, subsample=0.5; total time=  27.6s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=125, reg_alpha=1.0, reg_lambda=0.2, subsample=0.5; total time=  34.8s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=125, reg_alpha=1.0, reg_lambda=0.2, subsample=0.5; total time=  30.8s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=125, reg_alpha=1.0, reg_lambda=0.2, subsample=0.5; total time=  31.6s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=125, reg_alpha=1.0, reg_lambda=0.2, subsample=0.5; total time=  32.0s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.05, max_depth=2, n_estimators=150, reg_alpha=0.4, reg_lambda=0.0, subsample=0.5; total time=  22.0s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.05, max_depth=2, n_estimators=150, reg_alpha=0.4, reg_lambda=0.0, subsample=0.5; total time=  24.1s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.05, max_depth=2, n_estimators=150, reg_alpha=0.4, reg_lambda=0.0, subsample=0.5; total time=  23.4s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.7999999999999999, learning_rate=0.05, max_depth=2, n_estimators=150, reg_alpha=0.4, reg_lambda=0.0, subsample=0.5; total time=  23.3s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=125, reg_alpha=0.4, reg_lambda=0.8, subsample=0.5; total time=  27.1s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=125, reg_alpha=0.4, reg_lambda=0.8, subsample=0.5; total time=  25.7s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=125, reg_alpha=0.4, reg_lambda=0.8, subsample=0.5; total time=  24.1s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=125, reg_alpha=0.4, reg_lambda=0.8, subsample=0.5; total time=  24.6s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=2, n_estimators=125, reg_alpha=0.8, reg_lambda=1.0, subsample=0.5; total time= 7.8min\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=2, n_estimators=125, reg_alpha=0.8, reg_lambda=1.0, subsample=0.5; total time=  39.0s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=2, n_estimators=125, reg_alpha=0.8, reg_lambda=1.0, subsample=0.5; total time=  38.2s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=2, n_estimators=125, reg_alpha=0.8, reg_lambda=1.0, subsample=0.5; total time=  36.6s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=0.7, learning_rate=0.1, max_depth=2, n_estimators=125, reg_alpha=0.6000000000000001, reg_lambda=1.0, subsample=0.5; total time=  29.0s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=0.7, learning_rate=0.1, max_depth=2, n_estimators=125, reg_alpha=0.6000000000000001, reg_lambda=1.0, subsample=0.5; total time=  28.1s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=0.7, learning_rate=0.1, max_depth=2, n_estimators=125, reg_alpha=0.6000000000000001, reg_lambda=1.0, subsample=0.5; total time=  31.1s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=0.7, learning_rate=0.1, max_depth=2, n_estimators=125, reg_alpha=0.6000000000000001, reg_lambda=1.0, subsample=0.5; total time=  39.3s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=1.0999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=125, reg_alpha=0.0, reg_lambda=1.0, subsample=0.6; total time=   1.3s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=1.0999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=125, reg_alpha=0.0, reg_lambda=1.0, subsample=0.6; total time=   1.6s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=1.0999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=125, reg_alpha=0.0, reg_lambda=1.0, subsample=0.6; total time=   1.0s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=1.0999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=125, reg_alpha=0.0, reg_lambda=1.0, subsample=0.6; total time=   1.5s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=175, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.5; total time=  48.8s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=175, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.5; total time=  47.5s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=175, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.5; total time=  37.1s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=175, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.5; total time=  52.5s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=3, n_estimators=175, reg_alpha=1.0, reg_lambda=1.0, subsample=0.6; total time=  51.2s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=3, n_estimators=175, reg_alpha=1.0, reg_lambda=1.0, subsample=0.6; total time=  59.1s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=3, n_estimators=175, reg_alpha=1.0, reg_lambda=1.0, subsample=0.6; total time=  51.5s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=3, n_estimators=175, reg_alpha=1.0, reg_lambda=1.0, subsample=0.6; total time= 1.0min\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=175, reg_alpha=0.0, reg_lambda=0.6000000000000001, subsample=0.5; total time=  43.1s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=175, reg_alpha=0.0, reg_lambda=0.6000000000000001, subsample=0.5; total time=  39.6s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=175, reg_alpha=0.0, reg_lambda=0.6000000000000001, subsample=0.5; total time=  52.1s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=2, n_estimators=175, reg_alpha=0.0, reg_lambda=0.6000000000000001, subsample=0.5; total time= 1.2min\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=1.0999999999999999, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=0.8, reg_lambda=0.0, subsample=0.6; total time=   1.2s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=1.0999999999999999, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=0.8, reg_lambda=0.0, subsample=0.6; total time=   1.2s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=1.0999999999999999, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=0.8, reg_lambda=0.0, subsample=0.6; total time=   1.0s\n",
      "[CV] END colsample_bylevel=0.9999999999999999, colsample_bytree=1.0999999999999999, learning_rate=0.05, max_depth=2, n_estimators=125, reg_alpha=0.8, reg_lambda=0.0, subsample=0.6; total time=   1.1s\n",
      "Best parameters: {'subsample': 0.6, 'reg_lambda': 1.0, 'reg_alpha': 1.0, 'n_estimators': 175, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8999999999999999, 'colsample_bylevel': 0.7999999999999999}\n",
      "Lowest RMSE:  725.5316406916438\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_colsample_bylevel</th>\n",
       "      <th>param_reg_alpha</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>175</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-725.531641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-728.298138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>150</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-728.846952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>175</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-728.849778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>175</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-728.875567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>125</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-729.106870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>175</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-730.014044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>175</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-730.363107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>175</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-730.788005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-731.898524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>125</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-733.348913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-733.403620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>125</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-735.860925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>125</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-736.020014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-743.726688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>125</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-744.721150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>150</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-745.103170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>125</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-748.270828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>125</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>125</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_n_estimators param_learning_rate param_max_depth  \\\n",
       "17                175                 0.1               3   \n",
       "4                 150                 0.2               3   \n",
       "5                 150                0.15               3   \n",
       "6                 175                 0.2               3   \n",
       "7                 175                 0.2               3   \n",
       "10                125                0.15               3   \n",
       "18                175                0.15               2   \n",
       "16                175                0.15               2   \n",
       "3                 175                 0.2               3   \n",
       "2                 175                0.15               2   \n",
       "12                125                0.15               2   \n",
       "1                 125                0.05               3   \n",
       "13                125                 0.1               2   \n",
       "14                125                 0.1               2   \n",
       "0                 125                0.05               2   \n",
       "8                 125                0.05               2   \n",
       "11                150                0.05               2   \n",
       "9                 125                0.05               2   \n",
       "15                125                0.15               2   \n",
       "19                125                0.05               2   \n",
       "\n",
       "   param_colsample_bytree param_colsample_bylevel param_reg_alpha  \\\n",
       "17                    0.9                     0.8             1.0   \n",
       "4                     0.7                     0.7             0.6   \n",
       "5                     0.8                     0.9             0.6   \n",
       "6                     1.0                     0.8             0.8   \n",
       "7                     0.8                     0.6             0.0   \n",
       "10                    0.8                     0.8             1.0   \n",
       "18                    1.0                     1.0             0.0   \n",
       "16                    0.9                     0.7             0.8   \n",
       "3                     0.8                     0.7             0.6   \n",
       "2                     0.7                     0.6             1.0   \n",
       "12                    1.0                     0.8             0.4   \n",
       "1                     0.8                     1.0             1.0   \n",
       "13                    0.9                     0.7             0.8   \n",
       "14                    0.7                     1.0             0.6   \n",
       "0                     1.0                     0.7             1.2   \n",
       "8                     0.7                     0.9             0.8   \n",
       "11                    0.8                     0.6             0.4   \n",
       "9                     0.8                     0.6             0.0   \n",
       "15                    1.1                     0.8             0.0   \n",
       "19                    1.1                     1.0             0.8   \n",
       "\n",
       "   param_reg_lambda  mean_test_score  \n",
       "17              1.0      -725.531641  \n",
       "4               0.8      -728.298138  \n",
       "5               0.0      -728.846952  \n",
       "6               1.0      -728.849778  \n",
       "7               0.6      -728.875567  \n",
       "10              0.2      -729.106870  \n",
       "18              0.6      -730.014044  \n",
       "16              1.2      -730.363107  \n",
       "3               1.0      -730.788005  \n",
       "2               0.0      -731.898524  \n",
       "12              0.8      -733.348913  \n",
       "1               1.0      -733.403620  \n",
       "13              1.0      -735.860925  \n",
       "14              1.0      -736.020014  \n",
       "0               1.0      -743.726688  \n",
       "8               1.2      -744.721150  \n",
       "11              0.0      -745.103170  \n",
       "9               0.6      -748.270828  \n",
       "15              1.0              NaN  \n",
       "19              0.0              NaN  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = { 'max_depth': [2,3],\n",
    "    'learning_rate': np.arange(0.05,0.25,0.05),\n",
    "    'subsample': np.arange(0.5, 0.7, 0.1),\n",
    "    'colsample_bytree': np.arange(0.7, 1.1, 0.1),\n",
    "    'colsample_bylevel': np.arange(0.6, 1.1, 0.1),\n",
    "    'n_estimators': [125, 150, 175],\n",
    "    'reg_alpha': np.arange(0, 1.4, 0.2),\n",
    "    'reg_lambda': np.arange(0, 1.4, 0.2)}\n",
    "    \n",
    "xgbr = xgb.XGBRegressor()\n",
    "    \n",
    "xgb_search = RandomizedSearchCV(estimator=xgbr,\n",
    "                    cv=4,\n",
    "                    param_distributions=params,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    n_iter=20,\n",
    "                    verbose=2)\n",
    "                    \n",
    "xgb_search.fit(X_train.values, y_train_reg.values)\n",
    "                    \n",
    "print(\"Best parameters:\", xgb_search.best_params_)\n",
    "print(\"Lowest RMSE: \", (-xgb_search.best_score_))\n",
    "df = pd.DataFrame(xgb_search.cv_results_)\n",
    "\n",
    "df[['param_n_estimators',\n",
    "'param_learning_rate',\n",
    "'param_max_depth', \n",
    "'param_colsample_bytree', \n",
    "'param_colsample_bylevel',\n",
    "'param_reg_alpha',\n",
    "'param_reg_lambda',\n",
    "'mean_test_score']].sort_values(by ='mean_test_score', ascending=False)[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### III. Fine-Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=175, reg_alpha=1.2000000000000002, reg_lambda=1.8000000000000003, subsample=0.5; total time=  43.6s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=175, reg_alpha=1.2000000000000002, reg_lambda=1.8000000000000003, subsample=0.5; total time= 1.1min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=175, reg_alpha=1.2000000000000002, reg_lambda=1.8000000000000003, subsample=0.5; total time=  54.6s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=175, reg_alpha=1.2000000000000002, reg_lambda=1.8000000000000003, subsample=0.5; total time=  48.7s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=1.6000000000000005, reg_lambda=1.4000000000000004, subsample=0.6; total time=  45.6s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=1.6000000000000005, reg_lambda=1.4000000000000004, subsample=0.6; total time=  44.1s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=1.6000000000000005, reg_lambda=1.4000000000000004, subsample=0.6; total time=  45.9s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=1.6000000000000005, reg_lambda=1.4000000000000004, subsample=0.6; total time= 1.0min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=175, reg_alpha=1.4000000000000004, reg_lambda=1.0, subsample=0.7; total time=  51.9s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=175, reg_alpha=1.4000000000000004, reg_lambda=1.0, subsample=0.7; total time=  46.8s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=175, reg_alpha=1.4000000000000004, reg_lambda=1.0, subsample=0.7; total time=  50.3s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=175, reg_alpha=1.4000000000000004, reg_lambda=1.0, subsample=0.7; total time=  42.3s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=175, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.8999999999999999; total time=  44.8s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=175, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.8999999999999999; total time=  42.8s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=175, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.8999999999999999; total time=  41.5s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=175, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.8999999999999999; total time=  47.1s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=160, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.5; total time=  46.9s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=160, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.5; total time=  48.5s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=160, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.5; total time=  50.2s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=160, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.5; total time=  55.1s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=1.2000000000000002, reg_lambda=1.4000000000000004, subsample=0.5; total time=  57.0s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=1.2000000000000002, reg_lambda=1.4000000000000004, subsample=0.5; total time=  56.0s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=1.2000000000000002, reg_lambda=1.4000000000000004, subsample=0.5; total time=  57.1s\n",
      "[CV] END colsample_bylevel=0.7999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=1.2000000000000002, reg_lambda=1.4000000000000004, subsample=0.5; total time= 1.0min\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7999999999999999, learning_rate=0.1, max_depth=3, n_estimators=175, reg_alpha=1.6000000000000005, reg_lambda=1.4000000000000004, subsample=0.7; total time=  42.9s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7999999999999999, learning_rate=0.1, max_depth=3, n_estimators=175, reg_alpha=1.6000000000000005, reg_lambda=1.4000000000000004, subsample=0.7; total time= 1.5min\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7999999999999999, learning_rate=0.1, max_depth=3, n_estimators=175, reg_alpha=1.6000000000000005, reg_lambda=1.4000000000000004, subsample=0.7; total time=  42.8s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7999999999999999, learning_rate=0.1, max_depth=3, n_estimators=175, reg_alpha=1.6000000000000005, reg_lambda=1.4000000000000004, subsample=0.7; total time=  44.9s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=1.8000000000000003, reg_lambda=0.8, subsample=0.7; total time= 1.4min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=1.8000000000000003, reg_lambda=0.8, subsample=0.7; total time=  55.9s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=1.8000000000000003, reg_lambda=0.8, subsample=0.7; total time=  57.6s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=1.8000000000000003, reg_lambda=0.8, subsample=0.7; total time= 1.2min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.1, max_depth=3, n_estimators=160, reg_alpha=1.6000000000000005, reg_lambda=1.6000000000000005, subsample=0.8999999999999999; total time=  47.4s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.1, max_depth=3, n_estimators=160, reg_alpha=1.6000000000000005, reg_lambda=1.6000000000000005, subsample=0.8999999999999999; total time=  53.6s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.1, max_depth=3, n_estimators=160, reg_alpha=1.6000000000000005, reg_lambda=1.6000000000000005, subsample=0.8999999999999999; total time= 1.1min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.1, max_depth=3, n_estimators=160, reg_alpha=1.6000000000000005, reg_lambda=1.6000000000000005, subsample=0.8999999999999999; total time=  40.3s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=1.6000000000000005, reg_lambda=1.6000000000000005, subsample=0.7999999999999999; total time=  36.0s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=1.6000000000000005, reg_lambda=1.6000000000000005, subsample=0.7999999999999999; total time=  38.8s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=1.6000000000000005, reg_lambda=1.6000000000000005, subsample=0.7999999999999999; total time=  41.2s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=1.6000000000000005, reg_lambda=1.6000000000000005, subsample=0.7999999999999999; total time=  53.1s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=1.0, reg_lambda=1.0, subsample=0.5; total time=  37.5s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=1.0, reg_lambda=1.0, subsample=0.5; total time=  44.7s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=1.0, reg_lambda=1.0, subsample=0.5; total time=  42.0s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=1.0, reg_lambda=1.0, subsample=0.5; total time=  51.2s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=175, reg_alpha=1.8000000000000003, reg_lambda=0.6, subsample=0.6; total time=  55.8s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=175, reg_alpha=1.8000000000000003, reg_lambda=0.6, subsample=0.6; total time=  52.7s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=175, reg_alpha=1.8000000000000003, reg_lambda=0.6, subsample=0.6; total time= 1.4min\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=175, reg_alpha=1.8000000000000003, reg_lambda=0.6, subsample=0.6; total time=  47.7s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=0.8, reg_lambda=1.0, subsample=0.7; total time=  56.3s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=0.8, reg_lambda=1.0, subsample=0.7; total time=  58.6s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=0.8, reg_lambda=1.0, subsample=0.7; total time=  58.2s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=0.8, reg_lambda=1.0, subsample=0.7; total time= 1.2min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=1.2000000000000002, reg_lambda=0.8, subsample=0.8999999999999999; total time= 1.8min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=1.2000000000000002, reg_lambda=0.8, subsample=0.8999999999999999; total time= 1.4min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=1.2000000000000002, reg_lambda=0.8, subsample=0.8999999999999999; total time= 1.3min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.9999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=180, reg_alpha=1.2000000000000002, reg_lambda=0.8, subsample=0.8999999999999999; total time= 1.0min\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=3, n_estimators=160, reg_alpha=1.4000000000000004, reg_lambda=0.8, subsample=0.6; total time= 1.2min\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=3, n_estimators=160, reg_alpha=1.4000000000000004, reg_lambda=0.8, subsample=0.6; total time=  45.6s\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=3, n_estimators=160, reg_alpha=1.4000000000000004, reg_lambda=0.8, subsample=0.6; total time= 1.1min\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.8999999999999999, learning_rate=0.1, max_depth=3, n_estimators=160, reg_alpha=1.4000000000000004, reg_lambda=0.8, subsample=0.6; total time= 1.7min\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=1.6000000000000005, reg_lambda=1.6000000000000005, subsample=0.8999999999999999; total time= 1.1min\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=1.6000000000000005, reg_lambda=1.6000000000000005, subsample=0.8999999999999999; total time= 1.7min\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=1.6000000000000005, reg_lambda=1.6000000000000005, subsample=0.8999999999999999; total time= 1.5min\n",
      "[CV] END colsample_bylevel=0.7, colsample_bytree=0.7, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=1.6000000000000005, reg_lambda=1.6000000000000005, subsample=0.8999999999999999; total time=  39.6s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.9999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=0.6, reg_lambda=0.6, subsample=0.6; total time=  41.2s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.9999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=0.6, reg_lambda=0.6, subsample=0.6; total time=  40.5s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.9999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=0.6, reg_lambda=0.6, subsample=0.6; total time=  51.3s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.9999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=0.6, reg_lambda=0.6, subsample=0.6; total time= 1.6min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=0.6, reg_lambda=1.6000000000000005, subsample=0.8999999999999999; total time=  58.4s\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=0.6, reg_lambda=1.6000000000000005, subsample=0.8999999999999999; total time= 1.2min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=0.6, reg_lambda=1.6000000000000005, subsample=0.8999999999999999; total time= 1.2min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=0.6, reg_lambda=1.6000000000000005, subsample=0.8999999999999999; total time= 1.1min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.8999999999999999; total time= 1.3min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.8999999999999999; total time= 1.1min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.8999999999999999; total time= 1.4min\n",
      "[CV] END colsample_bylevel=0.8999999999999999, colsample_bytree=0.7999999999999999, learning_rate=0.15000000000000002, max_depth=3, n_estimators=160, reg_alpha=0.8, reg_lambda=1.2000000000000002, subsample=0.8999999999999999; total time= 1.8min\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=0.6, reg_lambda=1.4000000000000004, subsample=0.5; total time= 1.0min\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=0.6, reg_lambda=1.4000000000000004, subsample=0.5; total time=  50.3s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=0.6, reg_lambda=1.4000000000000004, subsample=0.5; total time=  51.8s\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8999999999999999, learning_rate=0.20000000000000004, max_depth=3, n_estimators=175, reg_alpha=0.6, reg_lambda=1.4000000000000004, subsample=0.5; total time=  57.2s\n",
      "Best parameters: {'subsample': 0.7999999999999999, 'reg_lambda': 1.6000000000000005, 'reg_alpha': 1.6000000000000005, 'n_estimators': 180, 'max_depth': 3, 'learning_rate': 0.15000000000000002, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.7}\n",
      "Lowest RMSE:  724.2346905025245\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_colsample_bylevel</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_reg_alpha</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>180</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-724.234691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>175</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-724.415245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>180</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-724.553677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>180</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-724.615069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>175</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-724.733863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>160</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-724.795149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>160</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-725.101038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>-725.772439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-726.085670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>175</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-726.183514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>175</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-726.286154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>160</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-726.463694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>175</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-726.602000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>160</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-726.733316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-726.750031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>160</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-727.118274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-727.198086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>175</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-728.250522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-728.270839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>175</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-729.539377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_n_estimators param_learning_rate param_max_depth  \\\n",
       "9                 180                0.15               3   \n",
       "3                 175                0.15               3   \n",
       "13                180                0.15               3   \n",
       "12                180                0.15               3   \n",
       "15                175                 0.2               3   \n",
       "17                160                0.15               3   \n",
       "18                160                0.15               3   \n",
       "0                 175                 0.1               3   \n",
       "2                 175                0.15               3   \n",
       "7                 175                 0.2               3   \n",
       "6                 175                 0.1               3   \n",
       "8                 160                 0.1               3   \n",
       "11                175                0.15               3   \n",
       "14                160                 0.1               3   \n",
       "5                 180                0.15               3   \n",
       "10                160                0.15               3   \n",
       "1                 160                0.15               3   \n",
       "16                175                 0.2               3   \n",
       "4                 160                 0.2               3   \n",
       "19                175                 0.2               3   \n",
       "\n",
       "   param_colsample_bytree param_colsample_bylevel param_subsample  \\\n",
       "9                     0.7                     0.7             0.8   \n",
       "3                     0.7                     0.9             0.9   \n",
       "13                    1.0                     0.9             0.9   \n",
       "12                    0.9                     0.7             0.7   \n",
       "15                    0.7                     0.7             0.9   \n",
       "17                    0.7                     0.9             0.9   \n",
       "18                    0.8                     0.9             0.9   \n",
       "0                     0.7                     0.9             0.5   \n",
       "2                     0.7                     0.9             0.7   \n",
       "7                     1.0                     0.9             0.7   \n",
       "6                     0.8                     0.7             0.7   \n",
       "8                     0.8                     0.9             0.9   \n",
       "11                    0.7                     0.7             0.6   \n",
       "14                    0.9                     0.7             0.6   \n",
       "5                     1.0                     0.8             0.5   \n",
       "10                    0.9                     0.6             0.5   \n",
       "1                     0.8                     0.7             0.6   \n",
       "16                    1.0                     0.6             0.6   \n",
       "4                     0.9                     0.7             0.5   \n",
       "19                    0.9                     0.6             0.5   \n",
       "\n",
       "   param_reg_alpha param_reg_lambda  mean_test_score  \n",
       "9              1.6              1.6      -724.234691  \n",
       "3              0.8              1.2      -724.415245  \n",
       "13             1.2              0.8      -724.553677  \n",
       "12             0.8              1.0      -724.615069  \n",
       "15             1.6              1.6      -724.733863  \n",
       "17             0.6              1.6      -724.795149  \n",
       "18             0.8              1.2      -725.101038  \n",
       "0              1.2              1.8      -725.772439  \n",
       "2              1.4              1.0      -726.085670  \n",
       "7              1.8              0.8      -726.183514  \n",
       "6              1.6              1.4      -726.286154  \n",
       "8              1.6              1.6      -726.463694  \n",
       "11             1.8              0.6      -726.602000  \n",
       "14             1.4              0.8      -726.733316  \n",
       "5              1.2              1.4      -726.750031  \n",
       "10             1.0              1.0      -727.118274  \n",
       "1              1.6              1.4      -727.198086  \n",
       "16             0.6              0.6      -728.250522  \n",
       "4              0.8              1.2      -728.270839  \n",
       "19             0.6              1.4      -729.539377  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = { 'max_depth': [3],\n",
    "    'learning_rate': np.arange(0.1,0.25,0.05),\n",
    "    'subsample': np.arange(0.5, 1, 0.1),\n",
    "    'colsample_bytree': np.arange(0.7, 1.0, 0.1),\n",
    "    'colsample_bylevel': np.arange(0.6, 1.0, 0.1),\n",
    "    'n_estimators': [160,175,180],\n",
    "    'reg_alpha': np.arange(0.6, 2, 0.2),\n",
    "    'reg_lambda': np.arange(0.6, 2, 0.2)}\n",
    "    \n",
    "xgbr = xgb.XGBRegressor()\n",
    "    \n",
    "xgb_search = RandomizedSearchCV(estimator=xgbr,\n",
    "                    cv=4,\n",
    "                    param_distributions=params,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    n_iter=20,\n",
    "                    verbose=2)\n",
    "                    \n",
    "xgb_search.fit(X_train.values, y_train_reg.values)\n",
    "                    \n",
    "print(\"Best parameters:\", xgb_search.best_params_)\n",
    "print(\"Lowest RMSE: \", (-xgb_search.best_score_))\n",
    "df = pd.DataFrame(xgb_search.cv_results_)\n",
    "\n",
    "df[['param_n_estimators',\n",
    "'param_learning_rate',\n",
    "'param_max_depth', \n",
    "'param_colsample_bytree', \n",
    "'param_colsample_bylevel',\n",
    "'param_subsample',\n",
    "'param_reg_alpha',\n",
    "'param_reg_lambda',\n",
    "'mean_test_score']].sort_values(by ='mean_test_score', ascending=False)[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "MSE ---- mean:  727.0034133606282 --- std:  12.483073056324466\n",
      "MAE ---- mean:  14.634427845175455 --- std:  0.12367212993625962\n",
      "MSLE ---- mean:  0.7357051896596116 --- std:  0.00488470090877944\n"
     ]
    }
   ],
   "source": [
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "# Initalize empty scores list\n",
    "scores = np.zeros(shape=(3,num_folds))\n",
    "\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Chosing Model\n",
    "    param_best = xgb_search.best_params_\n",
    "    model = xgb.XGBRegressor(**param_best)\n",
    "    model.fit(inputs[train], targets[train])\n",
    "\n",
    "    y_predict = model.predict(inputs[test])\n",
    "    y_predict_clipped = y_predict.clip(min=0) # setting min value to zero  (requirement for log_error)\n",
    "\n",
    "    # filling scores list with the chosen sco\n",
    "    scores[0,fold_no -1] = metrics.mean_squared_error(targets[test], y_predict)\n",
    "    scores[1,fold_no -1] = metrics.mean_absolute_error(targets[test], y_predict)\n",
    "    scores[2,fold_no -1] = metrics.mean_squared_log_error(targets[test], y_predict_clipped) # cannot handle negative input values as y_predict\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "scores_xgb = scores\n",
    "\n",
    "print('MSE ---- mean: ',scores_xgb[0].mean(), '--- std: ',scores_xgb[0].std())\n",
    "print('MAE ---- mean: ',scores_xgb[1].mean(), '--- std: ',scores_xgb[1].std())\n",
    "print('MSLE ---- mean: ',scores_xgb[2].mean(), '--- std: ',scores_xgb[2].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without transactional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "--------------WITHOUT TRANSACTIONAL FEATURES-----------\n",
      "MSE ---- mean:  745.4811622765876 --- std:  24.64498463839341\n",
      "MAE ---- mean:  14.943923209908766 --- std:  0.20587579588228366\n",
      "MSLE ---- mean:  0.7460473693360999 --- std:  0.006623128719123177\n"
     ]
    }
   ],
   "source": [
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "# Initalize empty scores list\n",
    "scores = np.zeros(shape=(3,num_folds))\n",
    "\n",
    "for train, test in kfold.split(inputs_0, targets):\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Chosing Model\n",
    "    param_best = xgb_search.best_params_\n",
    "    model = xgb.XGBRegressor(**param_best)\n",
    "    model.fit(inputs_0[train], targets[train])\n",
    "\n",
    "    y_predict = model.predict(inputs_0[test])\n",
    "    y_predict_clipped = y_predict.clip(min=0) # setting min value to zero  (requirement for log_error)\n",
    "\n",
    "    # filling scores list with the chosen sco\n",
    "    scores[0,fold_no -1] = metrics.mean_squared_error(targets[test], y_predict)\n",
    "    scores[1,fold_no -1] = metrics.mean_absolute_error(targets[test], y_predict)\n",
    "    scores[2,fold_no -1] = metrics.mean_squared_log_error(targets[test], y_predict_clipped) # cannot handle negative input values as y_predict\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "scores_xgb = scores\n",
    "print('--------------WITHOUT TRANSACTIONAL FEATURES-----------')\n",
    "print('MSE ---- mean: ',scores_xgb[0].mean(), '--- std: ',scores_xgb[0].std())\n",
    "print('MAE ---- mean: ',scores_xgb[1].mean(), '--- std: ',scores_xgb[1].std())\n",
    "print('MSLE ---- mean: ',scores_xgb[2].mean(), '--- std: ',scores_xgb[2].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06e87fc6394d8873b4ceac6ff7bcf39ba204d0a8f1170052b0425aec2106e483"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
